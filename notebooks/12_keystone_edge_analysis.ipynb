{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Experiment 12: Keystone Edge Analysis\n",
    "\n",
    "**Phase 4 - Identifying Critical Network Connections**\n",
    "\n",
    "## Background\n",
    "\n",
    "Experiment 8 showed an unexpected finding:\n",
    "- **Random fragmentation** creates 17.4% tip/recovery asymmetry\n",
    "- **Targeted high-betweenness removal** creates only 12.3% asymmetry\n",
    "\n",
    "This suggests that critical recovery pathways may be **distributed throughout the network** rather than concentrated in high-betweenness hubs.\n",
    "\n",
    "## Key Question\n",
    "\n",
    "**Which specific edges are most critical for maintaining recovery capacity?**\n",
    "\n",
    "We systematically remove individual edges and measure the impact on:\n",
    "1. Tip/recovery ratio (asymmetry)\n",
    "2. Recovery fraction\n",
    "3. Total entropy production\n",
    "\n",
    "## Experimental Design\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Network | 50-cell Amazon subnetwork |\n",
    "| Edges to test | Top 100 by flow (or all if fewer) |\n",
    "| Runs per edge removal | 10 |\n",
    "| Control (intact network) | 50 runs |\n",
    "| **Total simulations** | ~1,050 |\n",
    "\n",
    "## Expected Outputs\n",
    "\n",
    "1. Ranked list of \"keystone\" edges whose removal most impacts recovery\n",
    "2. Network visualization showing critical connections\n",
    "3. Edge characteristics that predict criticality (flow, betweenness, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/opt/research-local/src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from netCDF4 import Dataset\n",
    "from dask.distributed import as_completed\n",
    "from collections import defaultdict\n",
    "\n",
    "# Core energy-constrained module\n",
    "from energy_constrained import (\n",
    "    EnergyConstrainedNetwork,\n",
    "    EnergyConstrainedCusp,\n",
    "    GradientDrivenCoupling,\n",
    "    run_two_phase_experiment,\n",
    "    get_dask_client,\n",
    "    compute_network_metrics\n",
    ")\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dask-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Dask cluster\n",
    "client = get_dask_client()\n",
    "print(f\"Connected to: {client.scheduler_info()['address']}\")\n",
    "print(f\"Workers: {len(client.scheduler_info()['workers'])}\")\n",
    "print(f\"Total threads: {sum(w['nthreads'] for w in client.scheduler_info()['workers'].values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 2. Load Amazon Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/opt/research-local/data/amazon/amazon_adaptation_model/average_network/era5_new_network_data')\n",
    "\n",
    "def load_amazon_data(year=2003, months=[7, 8, 9]):\n",
    "    \"\"\"Load and average Amazon moisture recycling data.\"\"\"\n",
    "    all_rain = []\n",
    "    all_evap = []\n",
    "    all_network = []\n",
    "    \n",
    "    for month in months:\n",
    "        file_path = DATA_PATH / f'1deg_{year}_{month:02d}.nc'\n",
    "        if file_path.exists():\n",
    "            with Dataset(file_path, 'r') as ds:\n",
    "                all_rain.append(ds.variables['rain'][:])\n",
    "                all_evap.append(ds.variables['evap'][:])\n",
    "                all_network.append(ds.variables['network'][:])\n",
    "    \n",
    "    return {\n",
    "        'rain': np.mean(all_rain, axis=0),\n",
    "        'evap': np.mean(all_evap, axis=0),\n",
    "        'network': np.mean(all_network, axis=0),\n",
    "        'n_cells': len(all_rain[0])\n",
    "    }\n",
    "\n",
    "amazon_data = load_amazon_data(year=2003)\n",
    "print(f\"Loaded Amazon data: {amazon_data['n_cells']} cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 3. Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 12: Keystone Edge Analysis\n",
    "\n",
    "SWEEP_CONFIG = {\n",
    "    # Edge selection\n",
    "    'max_edges_to_test': 100,  # Test top N edges by flow\n",
    "    'n_runs_per_edge': 10,\n",
    "    'n_control_runs': 50,  # Baseline with intact network\n",
    "    \n",
    "    # Network parameters\n",
    "    'n_cells': 50,\n",
    "    'min_flow': 1.0,\n",
    "    'barrier_height': 0.2,\n",
    "    \n",
    "    # Two-phase simulation parameters\n",
    "    'cascade_duration': 200,\n",
    "    'recovery_duration': 800,\n",
    "    'dt': 0.5,\n",
    "    'cascade_sigma': 0.06,\n",
    "    'cascade_alpha': 1.5,\n",
    "    'recovery_sigma': 0.04,\n",
    "    'recovery_alpha': 2.0,\n",
    "    \n",
    "    # Seeds\n",
    "    'base_seed': 42,\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 12: KEYSTONE EDGE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Max edges to test: {SWEEP_CONFIG['max_edges_to_test']}\")\n",
    "print(f\"Runs per edge: {SWEEP_CONFIG['n_runs_per_edge']}\")\n",
    "print(f\"Control runs: {SWEEP_CONFIG['n_control_runs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "network-header",
   "metadata": {},
   "source": [
    "## 4. Network Creation and Edge Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "network-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_amazon_network_with_edge_info(data, config, seed=42):\n",
    "    \"\"\"\n",
    "    Create Amazon network and return edge information for analysis.\n",
    "    \n",
    "    Returns:\n",
    "        network: EnergyConstrainedNetwork\n",
    "        edge_info: list of dicts with edge properties\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    network_matrix = data['network']\n",
    "    n_cells = config['n_cells']\n",
    "    min_flow = config['min_flow']\n",
    "    barrier_height = config['barrier_height']\n",
    "    \n",
    "    total_flow = network_matrix.sum(axis=0) + network_matrix.sum(axis=1)\n",
    "    top_indices = np.argsort(total_flow)[-n_cells:]\n",
    "    \n",
    "    net = EnergyConstrainedNetwork()\n",
    "    \n",
    "    # Add elements\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        element = EnergyConstrainedCusp(\n",
    "            a=-1.0, b=1.0, c=0.0, x_0=0.0,\n",
    "            barrier_height=barrier_height,\n",
    "            dissipation_rate=0.1\n",
    "        )\n",
    "        net.add_element(f'cell_{i}', element)\n",
    "    \n",
    "    # Add couplings and track edge info\n",
    "    edge_info = []\n",
    "    for i, idx_i in enumerate(top_indices):\n",
    "        for j, idx_j in enumerate(top_indices):\n",
    "            if i != j:\n",
    "                flow = network_matrix[idx_i, idx_j]\n",
    "                if flow > min_flow:\n",
    "                    coupling = GradientDrivenCoupling(\n",
    "                        conductivity=flow / 100.0,\n",
    "                        state_coupling=0.1\n",
    "                    )\n",
    "                    net.add_coupling(f'cell_{i}', f'cell_{j}', coupling)\n",
    "                    \n",
    "                    edge_info.append({\n",
    "                        'source': f'cell_{i}',\n",
    "                        'target': f'cell_{j}',\n",
    "                        'source_idx': i,\n",
    "                        'target_idx': j,\n",
    "                        'flow': flow,\n",
    "                        'conductivity': flow / 100.0,\n",
    "                    })\n",
    "    \n",
    "    return net, edge_info, top_indices\n",
    "\n",
    "\n",
    "# Create baseline network\n",
    "baseline_network, edge_info, selected_cells = create_amazon_network_with_edge_info(\n",
    "    amazon_data, SWEEP_CONFIG\n",
    ")\n",
    "\n",
    "print(f\"Baseline network: {baseline_network.n_elements} nodes, {len(edge_info)} edges\")\n",
    "\n",
    "# Sort edges by flow\n",
    "edge_info_sorted = sorted(edge_info, key=lambda x: x['flow'], reverse=True)\n",
    "\n",
    "# Select edges to test\n",
    "n_edges_to_test = min(SWEEP_CONFIG['max_edges_to_test'], len(edge_info_sorted))\n",
    "edges_to_test = edge_info_sorted[:n_edges_to_test]\n",
    "\n",
    "print(f\"\\nEdges to test: {n_edges_to_test}\")\n",
    "print(f\"Flow range: {edges_to_test[-1]['flow']:.2f} - {edges_to_test[0]['flow']:.2f} mm/month\")\n",
    "\n",
    "total_sims = SWEEP_CONFIG['n_control_runs'] + n_edges_to_test * SWEEP_CONFIG['n_runs_per_edge']\n",
    "print(f\"\\nTotal simulations: {total_sims}\")\n",
    "print(f\"Estimated runtime: ~{total_sims * 2 / 60:.0f} minutes on 14 workers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-top-edges",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 10 edges by flow\n",
    "print(\"\\nTop 10 edges by moisture flow:\")\n",
    "print(f\"{'Rank':<6} {'Source':<10} {'Target':<10} {'Flow (mm)':<12}\")\n",
    "print(\"-\" * 40)\n",
    "for i, edge in enumerate(edges_to_test[:10]):\n",
    "    print(f\"{i+1:<6} {edge['source']:<10} {edge['target']:<10} {edge['flow']:<12.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worker-header",
   "metadata": {},
   "source": [
    "## 5. Worker Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worker-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_edge_removal_experiment(args):\n",
    "    \"\"\"\n",
    "    Worker function for single edge removal experiment.\n",
    "    \n",
    "    If edge_to_remove is None, runs with intact network (control).\n",
    "    \"\"\"\n",
    "    network_bytes, edge_to_remove, config, seed = args\n",
    "    \n",
    "    import sys\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import copy\n",
    "    \n",
    "    if '/opt/research-local/src' not in sys.path:\n",
    "        sys.path.insert(0, '/opt/research-local/src')\n",
    "    \n",
    "    from energy_constrained.solvers import run_two_phase_experiment\n",
    "    \n",
    "    # Reconstruct network\n",
    "    baseline_network = pickle.loads(network_bytes)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Remove edge if specified\n",
    "    if edge_to_remove is not None:\n",
    "        # Create a copy and remove the edge\n",
    "        network = copy.deepcopy(baseline_network)\n",
    "        source = edge_to_remove['source']\n",
    "        target = edge_to_remove['target']\n",
    "        try:\n",
    "            network.remove_coupling(source, target)\n",
    "        except:\n",
    "            pass  # Edge may not exist in this direction\n",
    "    else:\n",
    "        network = baseline_network\n",
    "    \n",
    "    # Run two-phase experiment\n",
    "    result = run_two_phase_experiment(\n",
    "        network=network,\n",
    "        cascade_duration=config['cascade_duration'],\n",
    "        recovery_duration=config['recovery_duration'],\n",
    "        dt=config['dt'],\n",
    "        cascade_sigma=config['cascade_sigma'],\n",
    "        cascade_alpha=config['cascade_alpha'],\n",
    "        recovery_sigma=config['recovery_sigma'],\n",
    "        recovery_alpha=config['recovery_alpha'],\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Compute metrics\n",
    "    n_cells = result.x_full.shape[1]\n",
    "    n_tip_events = 0\n",
    "    n_recover_events = 0\n",
    "    \n",
    "    for j in range(n_cells):\n",
    "        x_traj = result.x_full[:, j]\n",
    "        signs = np.sign(x_traj)\n",
    "        sign_changes = np.diff(signs)\n",
    "        n_tip_events += np.sum(sign_changes > 0)\n",
    "        n_recover_events += np.sum(sign_changes < 0)\n",
    "    \n",
    "    tip_recovery_ratio = n_tip_events / n_recover_events if n_recover_events > 0 else np.nan\n",
    "    \n",
    "    return {\n",
    "        'edge_source': edge_to_remove['source'] if edge_to_remove else 'control',\n",
    "        'edge_target': edge_to_remove['target'] if edge_to_remove else 'control',\n",
    "        'edge_flow': edge_to_remove['flow'] if edge_to_remove else 0,\n",
    "        'seed': seed,\n",
    "        'recovery_fraction': result.metrics['recovery_fraction'],\n",
    "        'n_tip_events': n_tip_events,\n",
    "        'n_recover_events': n_recover_events,\n",
    "        'tip_recovery_ratio': tip_recovery_ratio,\n",
    "        'n_permanent_tips': result.metrics['n_permanent_tips'],\n",
    "        'pct_tipped_cascade': result.metrics['pct_tipped_at_cascade_end'],\n",
    "        'final_pct_tipped': result.metrics['final_pct_tipped'],\n",
    "    }\n",
    "\n",
    "print(\"Worker function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-header",
   "metadata": {},
   "source": [
    "## 6. Run Edge Removal Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize and scatter baseline network\n",
    "network_bytes = pickle.dumps(baseline_network)\n",
    "print(f\"Network serialized: {len(network_bytes) / 1024:.1f} KB\")\n",
    "\n",
    "network_future = client.scatter(network_bytes, broadcast=True)\n",
    "print(\"Network broadcast to all workers\")\n",
    "\n",
    "# Build task arguments\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPERIMENT 12: Starting Keystone Edge Sweep\")\n",
    "print(\"=\" * 60)\n",
    "start_time = time.time()\n",
    "\n",
    "task_args = []\n",
    "\n",
    "# Control runs (intact network)\n",
    "for run_idx in range(SWEEP_CONFIG['n_control_runs']):\n",
    "    seed = SWEEP_CONFIG['base_seed'] + run_idx\n",
    "    task_args.append((network_bytes, None, SWEEP_CONFIG, seed))\n",
    "\n",
    "# Edge removal runs\n",
    "for edge_idx, edge in enumerate(edges_to_test):\n",
    "    for run_idx in range(SWEEP_CONFIG['n_runs_per_edge']):\n",
    "        seed = SWEEP_CONFIG['base_seed'] + 10000 + edge_idx * 100 + run_idx\n",
    "        task_args.append((network_bytes, edge, SWEEP_CONFIG, seed))\n",
    "\n",
    "print(f\"Generated {len(task_args)} task arguments\")\n",
    "print(f\"  Control runs: {SWEEP_CONFIG['n_control_runs']}\")\n",
    "print(f\"  Edge removal runs: {n_edges_to_test * SWEEP_CONFIG['n_runs_per_edge']}\")\n",
    "\n",
    "# Submit all tasks\n",
    "futures = client.map(run_edge_removal_experiment, task_args)\n",
    "print(f\"Submitted {len(futures)} tasks\")\n",
    "\n",
    "# Collect results\n",
    "all_results = []\n",
    "print(\"\\nProgress:\")\n",
    "for i, future in enumerate(as_completed(futures)):\n",
    "    result = future.result()\n",
    "    all_results.append(result)\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = (i + 1) / elapsed\n",
    "        remaining = (len(futures) - i - 1) / rate\n",
    "        print(f\"  Completed {i+1}/{len(futures)} ({100*(i+1)/len(futures):.1f}%) \"\n",
    "              f\"- {elapsed:.0f}s elapsed, ~{remaining:.0f}s remaining\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"COMPLETE: {len(all_results)} simulations in {elapsed:.1f}s ({elapsed/60:.1f} min)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## 7. Results Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_results)\n",
    "print(f\"Results shape: {df.shape}\")\n",
    "\n",
    "# Create edge identifier\n",
    "df['edge_id'] = df['edge_source'] + ' -> ' + df['edge_target']\n",
    "df.loc[df['edge_source'] == 'control', 'edge_id'] = 'control'\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by edge\n",
    "summary = df.groupby('edge_id').agg({\n",
    "    'edge_flow': 'first',\n",
    "    'recovery_fraction': ['mean', 'std'],\n",
    "    'tip_recovery_ratio': ['mean', 'std'],\n",
    "    'n_permanent_tips': ['mean', 'std'],\n",
    "    'n_tip_events': 'mean',\n",
    "    'n_recover_events': 'mean',\n",
    "}).round(4)\n",
    "\n",
    "summary.columns = ['_'.join(col) if col[1] else col[0] for col in summary.columns]\n",
    "summary = summary.reset_index()\n",
    "\n",
    "# Get control baseline\n",
    "control = summary[summary['edge_id'] == 'control'].iloc[0]\n",
    "control_recovery = control['recovery_fraction_mean']\n",
    "control_ratio = control['tip_recovery_ratio_mean']\n",
    "\n",
    "print(f\"Control (intact network):\")\n",
    "print(f\"  Recovery fraction: {control_recovery:.3f}\")\n",
    "print(f\"  Tip/recovery ratio: {control_ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impact-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate impact of each edge removal\n",
    "summary['recovery_impact'] = summary['recovery_fraction_mean'] - control_recovery\n",
    "summary['ratio_impact'] = summary['tip_recovery_ratio_mean'] - control_ratio\n",
    "\n",
    "# Rank edges by negative impact (most harmful removals first)\n",
    "edge_summary = summary[summary['edge_id'] != 'control'].copy()\n",
    "edge_summary = edge_summary.sort_values('recovery_impact', ascending=True)\n",
    "edge_summary['rank'] = range(1, len(edge_summary) + 1)\n",
    "\n",
    "print(\"\\nTop 20 KEYSTONE EDGES (most critical for recovery):\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Rank':<6} {'Edge':<25} {'Flow':<10} {'Rec Impact':<12} {'Ratio Impact':<12}\")\n",
    "print(\"-\" * 80)\n",
    "for _, row in edge_summary.head(20).iterrows():\n",
    "    print(f\"{row['rank']:<6} {row['edge_id']:<25} {row['edge_flow_first']:<10.1f} \"\n",
    "          f\"{row['recovery_impact']:<+12.3f} {row['ratio_impact']:<+12.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Panel 1: Recovery Impact Distribution\n",
    "ax = axes[0, 0]\n",
    "impacts = edge_summary['recovery_impact'].values\n",
    "ax.hist(impacts, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax.axvline(0, color='red', linestyle='--', linewidth=2, label='Control (no change)')\n",
    "ax.axvline(np.percentile(impacts, 10), color='orange', linestyle=':', \n",
    "           label=f'10th percentile ({np.percentile(impacts, 10):.3f})')\n",
    "ax.set_xlabel('Recovery Impact (vs Control)', fontsize=12)\n",
    "ax.set_ylabel('Number of Edges', fontsize=12)\n",
    "ax.set_title('Distribution of Edge Removal Impacts', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Impact vs Flow\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(edge_summary['edge_flow_first'], edge_summary['recovery_impact'],\n",
    "           c=edge_summary['rank'], cmap='RdYlGn', s=50, alpha=0.7)\n",
    "ax.axhline(0, color='gray', linestyle='--', alpha=0.7)\n",
    "ax.set_xlabel('Edge Flow (mm/month)', fontsize=12)\n",
    "ax.set_ylabel('Recovery Impact', fontsize=12)\n",
    "ax.set_title('Impact vs Moisture Flow', fontsize=14)\n",
    "\n",
    "# Highlight top 10 keystone edges\n",
    "top10 = edge_summary.head(10)\n",
    "ax.scatter(top10['edge_flow_first'], top10['recovery_impact'],\n",
    "           s=150, facecolors='none', edgecolors='red', linewidths=2,\n",
    "           label='Top 10 Keystone')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 3: Ranked Edge Impacts (bar chart)\n",
    "ax = axes[1, 0]\n",
    "top20 = edge_summary.head(20)\n",
    "y_pos = range(len(top20))\n",
    "colors = plt.cm.RdYlGn(np.linspace(0.1, 0.9, len(top20)))\n",
    "ax.barh(y_pos, top20['recovery_impact'], color=colors, edgecolor='black')\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([f\"{row['edge_id']} ({row['edge_flow_first']:.0f})\" \n",
    "                    for _, row in top20.iterrows()], fontsize=8)\n",
    "ax.axvline(0, color='red', linestyle='--')\n",
    "ax.set_xlabel('Recovery Impact', fontsize=12)\n",
    "ax.set_title('Top 20 Keystone Edges', fontsize=14)\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Panel 4: Asymmetry Impact (ratio change)\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(edge_summary['recovery_impact'], edge_summary['ratio_impact'],\n",
    "           c=edge_summary['edge_flow_first'], cmap='viridis', s=50, alpha=0.7)\n",
    "ax.axhline(0, color='gray', linestyle='--', alpha=0.7)\n",
    "ax.axvline(0, color='gray', linestyle='--', alpha=0.7)\n",
    "ax.set_xlabel('Recovery Impact', fontsize=12)\n",
    "ax.set_ylabel('Tip/Recovery Ratio Impact', fontsize=12)\n",
    "ax.set_title('Recovery vs Asymmetry Impact', fontsize=14)\n",
    "plt.colorbar(ax.collections[0], ax=ax, label='Edge Flow')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspace/data/exp12_keystone_edges.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nPlot saved to /workspace/data/exp12_keystone_edges.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pattern-header",
   "metadata": {},
   "source": [
    "## 9. Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pattern-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze patterns in keystone edges\n",
    "print(\"=\" * 70)\n",
    "print(\"KEYSTONE EDGE PATTERN ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Flow correlation\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "flow_corr, flow_p = pearsonr(edge_summary['edge_flow_first'], edge_summary['recovery_impact'])\n",
    "flow_spearman, flow_sp = spearmanr(edge_summary['edge_flow_first'], edge_summary['recovery_impact'])\n",
    "\n",
    "print(f\"\\n1. FLOW vs IMPACT CORRELATION:\")\n",
    "print(f\"   Pearson r = {flow_corr:.3f} (p = {flow_p:.4f})\")\n",
    "print(f\"   Spearman Ï = {flow_spearman:.3f} (p = {flow_sp:.4f})\")\n",
    "\n",
    "# 2. Node involvement analysis\n",
    "print(f\"\\n2. NODE INVOLVEMENT IN TOP 20 KEYSTONE EDGES:\")\n",
    "top20 = edge_summary.head(20)\n",
    "\n",
    "# Count node appearances\n",
    "node_counts = defaultdict(int)\n",
    "for _, row in top20.iterrows():\n",
    "    source = row['edge_id'].split(' -> ')[0]\n",
    "    target = row['edge_id'].split(' -> ')[1]\n",
    "    node_counts[source] += 1\n",
    "    node_counts[target] += 1\n",
    "\n",
    "sorted_nodes = sorted(node_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"   Most common nodes in keystone edges:\")\n",
    "for node, count in sorted_nodes[:10]:\n",
    "    print(f\"     {node}: {count} appearances\")\n",
    "\n",
    "# 3. Directional patterns\n",
    "print(f\"\\n3. FLOW DIRECTION ANALYSIS:\")\n",
    "# Check if high-impact edges tend to be in one direction\n",
    "keystone_flows = top20['edge_flow_first'].mean()\n",
    "other_flows = edge_summary.iloc[20:]['edge_flow_first'].mean()\n",
    "print(f\"   Mean flow of top 20 keystone edges: {keystone_flows:.1f} mm/month\")\n",
    "print(f\"   Mean flow of other edges: {other_flows:.1f} mm/month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connectivity-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connectivity impact matrix\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONNECTIVITY IMPACT MATRIX\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Build matrix of impacts\n",
    "n_cells = SWEEP_CONFIG['n_cells']\n",
    "impact_matrix = np.zeros((n_cells, n_cells))\n",
    "\n",
    "for _, row in edge_summary.iterrows():\n",
    "    if row['edge_id'] != 'control':\n",
    "        parts = row['edge_id'].split(' -> ')\n",
    "        source_idx = int(parts[0].replace('cell_', ''))\n",
    "        target_idx = int(parts[1].replace('cell_', ''))\n",
    "        impact_matrix[source_idx, target_idx] = row['recovery_impact']\n",
    "\n",
    "# Plot impact matrix\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(impact_matrix, cmap='RdBu', vmin=-0.1, vmax=0.1)\n",
    "ax.set_xlabel('Target Cell', fontsize=12)\n",
    "ax.set_ylabel('Source Cell', fontsize=12)\n",
    "ax.set_title('Edge Removal Impact Matrix\\n(Red = harmful, Blue = beneficial)', fontsize=14)\n",
    "plt.colorbar(im, ax=ax, label='Recovery Impact')\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspace/data/exp12_impact_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "findings-header",
   "metadata": {},
   "source": [
    "## 10. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT 12: KEY FINDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Statistics\n",
    "n_harmful = (edge_summary['recovery_impact'] < -0.01).sum()\n",
    "n_beneficial = (edge_summary['recovery_impact'] > 0.01).sum()\n",
    "n_neutral = len(edge_summary) - n_harmful - n_beneficial\n",
    "\n",
    "worst_edge = edge_summary.iloc[0]\n",
    "best_edge = edge_summary.iloc[-1]\n",
    "\n",
    "print(f\"\"\"\n",
    "1. OVERALL STATISTICS:\n",
    "   Edges tested: {len(edge_summary)}\n",
    "   Harmful removals (< -1%): {n_harmful} ({100*n_harmful/len(edge_summary):.1f}%)\n",
    "   Beneficial removals (> +1%): {n_beneficial} ({100*n_beneficial/len(edge_summary):.1f}%)\n",
    "   Neutral: {n_neutral} ({100*n_neutral/len(edge_summary):.1f}%)\n",
    "\n",
    "2. MOST CRITICAL EDGE (removing hurts recovery most):\n",
    "   Edge: {worst_edge['edge_id']}\n",
    "   Flow: {worst_edge['edge_flow_first']:.1f} mm/month\n",
    "   Recovery impact: {worst_edge['recovery_impact']:+.3f}\n",
    "\n",
    "3. LEAST CRITICAL EDGE (removing may help recovery):\n",
    "   Edge: {best_edge['edge_id']}\n",
    "   Flow: {best_edge['edge_flow_first']:.1f} mm/month\n",
    "   Recovery impact: {best_edge['recovery_impact']:+.3f}\n",
    "\n",
    "4. FLOW-IMPACT CORRELATION:\n",
    "   r = {flow_corr:.3f} (p = {flow_p:.4f})\n",
    "   {\"Strong\" if abs(flow_corr) > 0.5 else \"Moderate\" if abs(flow_corr) > 0.3 else \"Weak\"} correlation between flow and criticality\n",
    "\n",
    "5. CONSERVATION IMPLICATIONS:\n",
    "   Protecting the top 10 keystone edges would preserve:\n",
    "   - Maximum recovery potential\n",
    "   - Nodes involved: {', '.join([n for n, c in sorted_nodes[:5]])}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/workspace/data/experiment12_keystone_edges_full.csv', index=False)\n",
    "print(f\"Full results saved to /workspace/data/experiment12_keystone_edges_full.csv\")\n",
    "\n",
    "edge_summary.to_csv('/workspace/data/experiment12_keystone_edges_summary.csv', index=False)\n",
    "print(f\"Summary saved to /workspace/data/experiment12_keystone_edges_summary.csv\")\n",
    "\n",
    "# Save keystone ranking\n",
    "keystone_ranking = edge_summary[['rank', 'edge_id', 'edge_flow_first', \n",
    "                                  'recovery_impact', 'ratio_impact']].head(50)\n",
    "keystone_ranking.to_csv('/workspace/data/experiment12_keystone_ranking.csv', index=False)\n",
    "print(f\"Keystone ranking saved to /workspace/data/experiment12_keystone_ranking.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT 12 COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "CONFIGURATION:\n",
    "- Edges tested: {len(edge_summary)}\n",
    "- Runs per edge: {SWEEP_CONFIG['n_runs_per_edge']}\n",
    "- Control runs: {SWEEP_CONFIG['n_control_runs']}\n",
    "- Total simulations: {len(df)}\n",
    "- Runtime: {elapsed:.1f}s ({elapsed/60:.1f} min)\n",
    "\n",
    "FILES GENERATED:\n",
    "- /workspace/data/experiment12_keystone_edges_full.csv\n",
    "- /workspace/data/experiment12_keystone_edges_summary.csv\n",
    "- /workspace/data/experiment12_keystone_ranking.csv\n",
    "- /workspace/data/exp12_keystone_edges.png\n",
    "- /workspace/data/exp12_impact_matrix.png\n",
    "\n",
    "NEXT STEPS:\n",
    "- Use keystone ranking to prioritize conservation efforts\n",
    "- Test combined removal of multiple keystone edges\n",
    "- Map keystone edges to geographic locations\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

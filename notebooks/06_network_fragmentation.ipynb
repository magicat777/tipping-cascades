{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 8: Network Fragmentation and Deforestation Scenarios\n",
    "\n",
    "**Phase 4: Analysis and Validation**\n",
    "\n",
    "**Author:** Jason Holt  \n",
    "**Date:** December 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Test whether progressive removal of network connections causes thermodynamic buffering to collapse, and identify critical connectivity thresholds.\n",
    "\n",
    "## Scientific Rationale\n",
    "\n",
    "Phase 3 found that the Amazon moisture recycling network provides scale-invariant buffering (tip/recovery ratio ≈ 1.0). This suggests network connectivity is essential for resilience. Deforestation breaks moisture recycling pathways—if buffering depends on connectivity, there may be a critical threshold below which resilience collapses catastrophically.\n",
    "\n",
    "## Key Questions\n",
    "\n",
    "1. At what connectivity threshold does thermodynamic buffering collapse?\n",
    "2. Does the fragmentation method (random vs targeted) affect the threshold?\n",
    "3. Are there \"keystone\" connections whose removal is catastrophic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add src to path - use k3s mount path\n",
    "research_path = '/opt/research-local/src'\n",
    "if research_path not in sys.path:\n",
    "    sys.path.insert(0, research_path)\n",
    "\n",
    "# Import energy-constrained module\n",
    "from energy_constrained import (\n",
    "    EnergyConstrainedNetwork,\n",
    "    EnergyConstrainedCusp,\n",
    "    GradientDrivenCoupling,\n",
    "    EnergyAnalyzer,\n",
    "    fragment_network,\n",
    "    compute_network_metrics,\n",
    "    run_fragmentation_sweep,\n",
    "    get_dask_client,\n",
    "    run_ensemble_parallel,\n",
    "    results_to_solver_results,\n",
    "    DASK_SUPPORT\n",
    ")\n",
    "\n",
    "print(f\"Dask support: {DASK_SUPPORT}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Dask client\n",
    "dask_client = None\n",
    "if DASK_SUPPORT:\n",
    "    try:\n",
    "        dask_client = get_dask_client()\n",
    "        if dask_client:\n",
    "            print(f\"Dask dashboard: {dask_client.dashboard_link}\")\n",
    "            print(f\"Connected to {len(dask_client.scheduler_info()['workers'])} Dask workers\")\n",
    "    except Exception as e:\n",
    "        print(f\"Dask initialization failed: {e}\")\n",
    "        dask_client = None\n",
    "\n",
    "print(\"\\nDask cluster ready for parallel ensemble runs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Load Amazon Network Data\n",
    "\n",
    "We'll use the 50-cell Amazon subnetwork from Phase 3 as our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Amazon moisture recycling data\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "DATA_PATH = Path('/opt/research-local/data/amazon/amazon_adaptation_model/average_network/era5_new_network_data')\n",
    "\n",
    "def load_amazon_data(year=2003, months=[7, 8, 9]):\n",
    "    \"\"\"Load and average Amazon moisture recycling data for specified months.\"\"\"\n",
    "    all_rain = []\n",
    "    all_evap = []\n",
    "    all_network = []\n",
    "    \n",
    "    for month in months:\n",
    "        file_path = DATA_PATH / f'1deg_{year}_{month:02d}.nc'\n",
    "        if file_path.exists():\n",
    "            with Dataset(file_path, 'r') as ds:\n",
    "                all_rain.append(ds.variables['rain'][:])\n",
    "                all_evap.append(ds.variables['evap'][:])\n",
    "                all_network.append(ds.variables['network'][:])\n",
    "    \n",
    "    return {\n",
    "        'rain': np.mean(all_rain, axis=0),\n",
    "        'evap': np.mean(all_evap, axis=0),\n",
    "        'network': np.mean(all_network, axis=0),\n",
    "        'n_cells': len(all_rain[0])\n",
    "    }\n",
    "\n",
    "# Load 2003 (normal year) data\n",
    "amazon_data = load_amazon_data(year=2003)\n",
    "print(f\"Loaded Amazon data: {amazon_data['n_cells']} cells\")\n",
    "print(f\"Network shape: {amazon_data['network'].shape}\")\n",
    "print(f\"Rain range: {amazon_data['rain'].min():.1f} - {amazon_data['rain'].max():.1f} mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_amazon_network(data, n_cells=50, min_flow=1.0):\n    \"\"\"\n    Create EnergyConstrainedNetwork from Amazon moisture recycling data.\n    \n    Parameters\n    ----------\n    data : dict\n        Amazon data with 'rain', 'evap', 'network' arrays\n    n_cells : int\n        Number of cells to include (top by connectivity)\n    min_flow : float\n        Minimum moisture flow to create edge (mm/month)\n        \n    Returns\n    -------\n    EnergyConstrainedNetwork\n        Network ready for simulation\n    \"\"\"\n    network_matrix = data['network']\n    rain = data['rain']\n    evap = data['evap']\n    \n    # Select top cells by total connectivity\n    total_flow = network_matrix.sum(axis=0) + network_matrix.sum(axis=1)\n    top_indices = np.argsort(total_flow)[-n_cells:]\n    \n    # Create network\n    net = EnergyConstrainedNetwork()\n    \n    # Add elements with barrier heights based on rain/evap ratio\n    for i, idx in enumerate(top_indices):\n        # Barrier height: lower rain/evap = more vulnerable\n        ratio = min(rain[idx] / max(evap[idx], 1), 2.0)\n        barrier_height = 0.3 + 0.4 * min(ratio / 2, 1.0)\n        \n        # Use correct EnergyConstrainedCusp parameters\n        element = EnergyConstrainedCusp(\n            a=-1.0,           # Cusp parameter a\n            b=1.0,            # Cusp parameter b\n            c=0.0,            # Cusp parameter c\n            x_0=0.0,          # Initial state\n            barrier_height=barrier_height,\n            dissipation_rate=0.1\n        )\n        net.add_element(f'cell_{i}', element)\n    \n    # Add couplings based on moisture flow\n    # GradientDrivenCoupling uses conductivity (not strength)\n    n_edges = 0\n    for i, idx_i in enumerate(top_indices):\n        for j, idx_j in enumerate(top_indices):\n            if i != j:\n                flow = network_matrix[idx_i, idx_j]\n                if flow > min_flow:\n                    # Scale conductivity by flow magnitude\n                    coupling = GradientDrivenCoupling(\n                        conductivity=flow / 100.0,\n                        state_coupling=0.1\n                    )\n                    net.add_coupling(f'cell_{i}', f'cell_{j}', coupling)\n                    n_edges += 1\n    \n    print(f\"Created network: {net.n_elements} nodes, {n_edges} edges\")\n    return net\n\n# Create baseline 50-cell network\nbaseline_network = create_amazon_network(amazon_data, n_cells=50, min_flow=1.0)\nbaseline_metrics = compute_network_metrics(baseline_network)\nprint(f\"\\nBaseline network metrics:\")\nfor k, v in baseline_metrics.items():\n    print(f\"  {k}: {v:.4f}\" if isinstance(v, float) else f\"  {k}: {v}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Test Fragmentation Function\n",
    "\n",
    "Verify the fragmentation function works correctly before running experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test fragmentation at different levels\n",
    "test_levels = [1.0, 0.75, 0.50, 0.25, 0.10]\n",
    "\n",
    "print(\"Testing network fragmentation:\")\n",
    "print(f\"{'Retention':<12} {'Nodes':<8} {'Edges':<8} {'Density':<10} {'Components':<12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for retention in test_levels:\n",
    "    fragmented = fragment_network(baseline_network, retention, method='random', seed=42)\n",
    "    metrics = compute_network_metrics(fragmented)\n",
    "    print(f\"{retention:<12.0%} {metrics['n_nodes']:<8} {metrics['n_edges']:<8} {metrics['density']:<10.4f} {metrics['n_components']:<12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different fragmentation methods\n",
    "methods = ['random', 'low_flow_first', 'high_betweenness_first']\n",
    "retention = 0.5\n",
    "\n",
    "print(f\"\\nFragmentation methods at {retention:.0%} retention:\")\n",
    "print(f\"{'Method':<25} {'Edges':<8} {'Density':<10} {'Clustering':<12} {'Components':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for method in methods:\n",
    "    fragmented = fragment_network(baseline_network, retention, method=method, seed=42)\n",
    "    metrics = compute_network_metrics(fragmented)\n",
    "    print(f\"{method:<25} {metrics['n_edges']:<8} {metrics['density']:<10.4f} {metrics['clustering']:<12.4f} {metrics['n_components']:<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Fragmentation Experiment Configuration\n",
    "\n",
    "Define the experimental parameters for the fragmentation sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "FRAGMENTATION_CONFIG = {\n",
    "    'retention_fractions': [1.0, 0.90, 0.75, 0.50, 0.25, 0.10],\n",
    "    'methods': ['random', 'high_betweenness_first'],\n",
    "    'n_fragmentation_replicates': 3,  # Random fragmentation replicates\n",
    "    'n_simulation_runs': 10,  # Ensemble size per network\n",
    "    'duration': 500.0,\n",
    "    'dt': 0.5,\n",
    "    'sigma': 0.06,\n",
    "    'alpha': 1.5,  # Lévy noise (cascade-triggering)\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Calculate total simulations\n",
    "n_levels = len(FRAGMENTATION_CONFIG['retention_fractions'])\n",
    "n_methods = len(FRAGMENTATION_CONFIG['methods'])\n",
    "n_frag_reps = FRAGMENTATION_CONFIG['n_fragmentation_replicates']\n",
    "n_sim_runs = FRAGMENTATION_CONFIG['n_simulation_runs']\n",
    "\n",
    "total_sims = n_levels * n_methods * n_frag_reps * n_sim_runs\n",
    "print(f\"Experiment Configuration:\")\n",
    "print(f\"  Retention levels: {n_levels}\")\n",
    "print(f\"  Fragmentation methods: {n_methods}\")\n",
    "print(f\"  Fragmentation replicates: {n_frag_reps}\")\n",
    "print(f\"  Simulation runs per network: {n_sim_runs}\")\n",
    "print(f\"  Total simulations: {total_sims}\")\n",
    "print(f\"  \" + \"=\"*40)\n",
    "print(f\"  Noise: σ={FRAGMENTATION_CONFIG['sigma']}, α={FRAGMENTATION_CONFIG['alpha']} (Lévy)\")\n",
    "print(f\"  Duration: {FRAGMENTATION_CONFIG['duration']} time units\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Run Fragmentation Sweep Experiment\n",
    "\n",
    "Run simulations across all fragmentation levels and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_fragmentation_experiment(baseline_net, config):\n    \"\"\"\n    Run complete fragmentation experiment.\n    \n    Returns DataFrame with results.\n    \"\"\"\n    results = []\n    \n    for method in config['methods']:\n        print(f\"\\n{'='*60}\")\n        print(f\"Method: {method}\")\n        print(f\"{'='*60}\")\n        \n        for retention in config['retention_fractions']:\n            print(f\"\\n  Retention: {retention:.0%}\")\n            \n            for rep in range(config['n_fragmentation_replicates']):\n                # Create fragmented network\n                frag_seed = config['seed'] + rep * 1000\n                \n                if retention == 1.0:\n                    # Use baseline network (no fragmentation)\n                    network = baseline_net\n                    net_metrics = compute_network_metrics(network)\n                else:\n                    network = fragment_network(\n                        baseline_net,\n                        retention_fraction=retention,\n                        method=method,\n                        seed=frag_seed\n                    )\n                    net_metrics = compute_network_metrics(network)\n                \n                # Run ensemble\n                sim_seed = frag_seed + rep\n                \n                if dask_client:\n                    ensemble_results = run_ensemble_parallel(\n                        network,\n                        n_runs=config['n_simulation_runs'],\n                        duration=config['duration'],\n                        dt=config['dt'],\n                        sigma=config['sigma'],\n                        alpha=config['alpha'],\n                        seed=sim_seed\n                    )\n                else:\n                    from energy_constrained.solvers import run_ensemble\n                    ensemble_results = run_ensemble(\n                        network,\n                        n_runs=config['n_simulation_runs'],\n                        duration=config['duration'],\n                        dt=config['dt'],\n                        sigma=config['sigma'],\n                        alpha=config['alpha'],\n                        seed=sim_seed\n                    )\n                    ensemble_results = [\n                        {'run_idx': i, 't': r.t, 'x': r.x, 'E': r.E, \n                         'y': r.y, 'diagnostics': r.diagnostics}\n                        for i, r in enumerate(ensemble_results)\n                    ]\n                \n                # Convert to solver results\n                solver_results = results_to_solver_results(ensemble_results) if results_to_solver_results else ensemble_results\n                \n                # Aggregate metrics across ensemble\n                total_entropy = 0\n                n_tip_events = 0\n                n_recovery_events = 0\n                pct_tipped = 0\n                \n                for result in solver_results:\n                    # Create analyzer for each result (requires both network and result)\n                    analyzer = EnergyAnalyzer(network, result)\n                    \n                    # Get total entropy\n                    total_entropy += analyzer.compute_total_entropy_produced()\n                    \n                    # Count tipping events (TippingEvent is a dataclass with .direction attribute)\n                    events = analyzer.identify_tipping_events()\n                    for event in events:\n                        if event.direction == 'tip':\n                            n_tip_events += 1\n                        else:\n                            n_recovery_events += 1\n                    \n                    # Percent time tipped\n                    x_final = result.x[-1] if hasattr(result, 'x') else result['x'][-1]\n                    pct_tipped += np.mean(x_final > 0) * 100\n                \n                # Average across ensemble\n                n_runs = config['n_simulation_runs']\n                avg_entropy = total_entropy / n_runs\n                avg_tip_events = n_tip_events / n_runs\n                avg_recovery_events = n_recovery_events / n_runs\n                avg_pct_tipped = pct_tipped / n_runs\n                \n                # Tip/recovery ratio based on EVENT COUNTS (not entropy)\n                # This is more robust and meaningful\n                if n_recovery_events > 0:\n                    tip_recovery_ratio = n_tip_events / n_recovery_events\n                else:\n                    tip_recovery_ratio = np.nan\n                \n                # Store results\n                results.append({\n                    'method': method,\n                    'retention': retention,\n                    'replicate': rep,\n                    'n_edges': net_metrics['n_edges'],\n                    'density': net_metrics['density'],\n                    'n_components': net_metrics['n_components'],\n                    'largest_component': net_metrics['largest_component_fraction'],\n                    'avg_entropy': avg_entropy,\n                    'avg_tip_events': avg_tip_events,\n                    'avg_recovery_events': avg_recovery_events,\n                    'tip_recovery_ratio': tip_recovery_ratio,\n                    'avg_pct_tipped': avg_pct_tipped\n                })\n                \n                print(f\"    Rep {rep}: edges={net_metrics['n_edges']}, \"\n                      f\"entropy={avg_entropy:.1f}, tip/rec={tip_recovery_ratio:.3f}\")\n    \n    return pd.DataFrame(results)\n\nprint(\"Starting fragmentation experiment...\")\nprint(f\"Total simulations: {total_sims}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "fragmentation_results = run_fragmentation_experiment(baseline_network, FRAGMENTATION_CONFIG)\n",
    "\n",
    "print(f\"\\n\\nExperiment complete!\")\n",
    "print(f\"Results shape: {fragmentation_results.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by retention level and method\n",
    "summary = fragmentation_results.groupby(['method', 'retention']).agg({\n",
    "    'n_edges': 'mean',\n",
    "    'avg_entropy': ['mean', 'std'],\n",
    "    'tip_recovery_ratio': ['mean', 'std'],\n",
    "    'avg_pct_tipped': ['mean', 'std'],\n",
    "    'n_components': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "print(\"Summary Statistics:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tip/recovery ratio vs retention\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "methods = fragmentation_results['method'].unique()\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "# Plot 1: Tip/Recovery Ratio\n",
    "ax = axes[0]\n",
    "for i, method in enumerate(methods):\n",
    "    data = fragmentation_results[fragmentation_results['method'] == method]\n",
    "    grouped = data.groupby('retention')['tip_recovery_ratio'].agg(['mean', 'std'])\n",
    "    ax.errorbar(grouped.index * 100, grouped['mean'], yerr=grouped['std'],\n",
    "                marker='o', label=method, color=colors[i], capsize=3)\n",
    "\n",
    "ax.axhline(y=1.0, color='gray', linestyle='--', label='Symmetric (ratio=1)')\n",
    "ax.set_xlabel('Edge Retention (%)')\n",
    "ax.set_ylabel('Tip/Recovery Entropy Ratio')\n",
    "ax.set_title('Thermodynamic Asymmetry vs Fragmentation')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 105)\n",
    "\n",
    "# Plot 2: Total Entropy\n",
    "ax = axes[1]\n",
    "for i, method in enumerate(methods):\n",
    "    data = fragmentation_results[fragmentation_results['method'] == method]\n",
    "    grouped = data.groupby('retention')['avg_entropy'].agg(['mean', 'std'])\n",
    "    ax.errorbar(grouped.index * 100, grouped['mean'], yerr=grouped['std'],\n",
    "                marker='o', label=method, color=colors[i], capsize=3)\n",
    "\n",
    "ax.set_xlabel('Edge Retention (%)')\n",
    "ax.set_ylabel('Total Entropy')\n",
    "ax.set_title('Entropy Production vs Fragmentation')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 105)\n",
    "\n",
    "# Plot 3: Percent Tipped\n",
    "ax = axes[2]\n",
    "for i, method in enumerate(methods):\n",
    "    data = fragmentation_results[fragmentation_results['method'] == method]\n",
    "    grouped = data.groupby('retention')['avg_pct_tipped'].agg(['mean', 'std'])\n",
    "    ax.errorbar(grouped.index * 100, grouped['mean'], yerr=grouped['std'],\n",
    "                marker='o', label=method, color=colors[i], capsize=3)\n",
    "\n",
    "ax.set_xlabel('Edge Retention (%)')\n",
    "ax.set_ylabel('Cells Tipped (%)')\n",
    "ax.set_title('Cascade Extent vs Fragmentation')\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 105)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspace/data/fragmentation_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to /workspace/data/fragmentation_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Identify Critical Threshold\n",
    "\n",
    "Find the fragmentation level where buffering collapses (tip/recovery ratio significantly > 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find threshold where ratio significantly exceeds 1\n",
    "THRESHOLD_CRITERION = 1.5  # Ratio above which buffering is \"collapsed\"\n",
    "\n",
    "print(f\"Critical threshold analysis (criterion: ratio > {THRESHOLD_CRITERION})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for method in methods:\n",
    "    data = fragmentation_results[fragmentation_results['method'] == method]\n",
    "    grouped = data.groupby('retention')['tip_recovery_ratio'].mean()\n",
    "    \n",
    "    # Find first retention level where ratio exceeds threshold\n",
    "    exceeded = grouped[grouped > THRESHOLD_CRITERION]\n",
    "    if len(exceeded) > 0:\n",
    "        critical = exceeded.index.max()  # Highest retention where threshold exceeded\n",
    "        print(f\"\\n{method}:\")\n",
    "        print(f\"  Critical threshold: {critical:.0%} retention\")\n",
    "        print(f\"  Ratio at threshold: {grouped[critical]:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\n{method}: Buffering maintained at all levels (ratio never > {THRESHOLD_CRITERION})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 8: NETWORK FRAGMENTATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Baseline vs most fragmented\n",
    "baseline = fragmentation_results[fragmentation_results['retention'] == 1.0]\n",
    "most_fragmented = fragmentation_results[fragmentation_results['retention'] == 0.10]\n",
    "\n",
    "print(f\"\\nBaseline (100% retention):\")\n",
    "print(f\"  Edges: {baseline['n_edges'].mean():.0f}\")\n",
    "print(f\"  Tip/Recovery Ratio: {baseline['tip_recovery_ratio'].mean():.3f}\")\n",
    "print(f\"  Total Entropy: {baseline['avg_entropy'].mean():.1f}\")\n",
    "print(f\"  % Tipped: {baseline['avg_pct_tipped'].mean():.1f}%\")\n",
    "\n",
    "print(f\"\\nMost Fragmented (10% retention):\")\n",
    "print(f\"  Edges: {most_fragmented['n_edges'].mean():.0f}\")\n",
    "print(f\"  Tip/Recovery Ratio: {most_fragmented['tip_recovery_ratio'].mean():.3f}\")\n",
    "print(f\"  Total Entropy: {most_fragmented['avg_entropy'].mean():.1f}\")\n",
    "print(f\"  % Tipped: {most_fragmented['avg_pct_tipped'].mean():.1f}%\")\n",
    "\n",
    "# Change\n",
    "ratio_change = most_fragmented['tip_recovery_ratio'].mean() / baseline['tip_recovery_ratio'].mean()\n",
    "entropy_change = (most_fragmented['avg_entropy'].mean() - baseline['avg_entropy'].mean()) / baseline['avg_entropy'].mean() * 100\n",
    "\n",
    "print(f\"\\nChange at 10% fragmentation:\")\n",
    "print(f\"  Tip/Recovery Ratio: {ratio_change:.1f}x\")\n",
    "print(f\"  Entropy: {entropy_change:+.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "output_path = '/workspace/data/experiment8_fragmentation_results.csv'\n",
    "fragmentation_results.to_csv(output_path, index=False)\n",
    "print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **[To be filled after running experiment]**\n",
    "\n",
    "2. **[To be filled after running experiment]**\n",
    "\n",
    "3. **[To be filled after running experiment]**\n",
    "\n",
    "### Implications for Conservation\n",
    "\n",
    "- [To be filled after analyzing results]\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment 9: Recovery Dynamics\n",
    "- Experiment 10: α-Sweep for Lévy-Gaussian transition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
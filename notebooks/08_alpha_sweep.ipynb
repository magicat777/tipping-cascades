{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Experiment 10: Alpha-Sweep - Mapping the Levy-Gaussian Transition\n",
    "\n",
    "**Phase 4 - Testing the Critical Alpha Hypothesis**\n",
    "\n",
    "## Background\n",
    "\n",
    "Experiment 9 revealed that **noise regime asymmetry** is the dominant source of hysteresis:\n",
    "- Cascade phase: Levy noise (alpha=1.5) with fat-tailed extreme events\n",
    "- Recovery phase: Gaussian noise (alpha=2.0) with bounded perturbations\n",
    "- Result: ~0% recovery because Gaussian noise lacks the extreme jumps needed to escape tipped state\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "There exists a **critical alpha value** (approximately 1.7) that separates:\n",
    "- **Levy regime** (alpha < 1.7): Heavy-tailed noise enables barrier crossing and recovery\n",
    "- **Gaussian regime** (alpha >= 1.7): Bounded noise traps system in tipped state\n",
    "\n",
    "## This Experiment\n",
    "\n",
    "Systematically sweep recovery alpha from 1.1 to 2.0 to:\n",
    "1. Map recovery fraction as a function of alpha\n",
    "2. Identify the critical alpha threshold\n",
    "3. Characterize whether the transition is sharp or gradual\n",
    "4. Measure thermodynamic signatures (entropy production) across regimes\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Alpha values | 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0 |\n",
    "| Cascade alpha (fixed) | 1.5 |\n",
    "| Ensemble runs | 30 per alpha |\n",
    "| Total simulations | 300 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/opt/research-local/src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from netCDF4 import Dataset\n",
    "from dask.distributed import as_completed\n",
    "\n",
    "# Core energy-constrained module\n",
    "from energy_constrained import (\n",
    "    EnergyConstrainedNetwork,\n",
    "    EnergyConstrainedCusp,\n",
    "    GradientDrivenCoupling,\n",
    "    run_two_phase_experiment,\n",
    "    EnergyAnalyzer,\n",
    "    get_dask_client\n",
    ")\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dask-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Dask cluster\n",
    "client = get_dask_client()\n",
    "print(f\"Connected to: {client.scheduler_info()['address']}\")\n",
    "print(f\"Workers: {len(client.scheduler_info()['workers'])}\")\n",
    "print(f\"Total threads: {sum(w['nthreads'] for w in client.scheduler_info()['workers'].values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 2. Load Amazon Moisture Recycling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/opt/research-local/data/amazon/amazon_adaptation_model/average_network/era5_new_network_data')\n",
    "\n",
    "def load_amazon_data(year=2003, months=[7, 8, 9]):\n",
    "    \"\"\"Load and average Amazon moisture recycling data for specified months.\"\"\"\n",
    "    all_rain = []\n",
    "    all_evap = []\n",
    "    all_network = []\n",
    "    \n",
    "    for month in months:\n",
    "        file_path = DATA_PATH / f'1deg_{year}_{month:02d}.nc'\n",
    "        if file_path.exists():\n",
    "            with Dataset(file_path, 'r') as ds:\n",
    "                all_rain.append(ds.variables['rain'][:])\n",
    "                all_evap.append(ds.variables['evap'][:])\n",
    "                all_network.append(ds.variables['network'][:])\n",
    "    \n",
    "    return {\n",
    "        'rain': np.mean(all_rain, axis=0),\n",
    "        'evap': np.mean(all_evap, axis=0),\n",
    "        'network': np.mean(all_network, axis=0),\n",
    "        'n_cells': len(all_rain[0])\n",
    "    }\n",
    "\n",
    "# Load 2003 (normal year) data\n",
    "amazon_data = load_amazon_data(year=2003)\n",
    "print(f\"Loaded Amazon data: {amazon_data['n_cells']} cells\")\n",
    "print(f\"Network shape: {amazon_data['network'].shape}\")\n",
    "print(f\"Rain range: {amazon_data['rain'].min():.1f} - {amazon_data['rain'].max():.1f} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 3. Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 10: Alpha-Sweep Configuration\n",
    "# Tests how recovery changes as a function of noise stability parameter alpha\n",
    "\n",
    "SWEEP_CONFIG = {\n",
    "    # Alpha sweep parameters\n",
    "    'alpha_values': np.round(np.arange(1.1, 2.05, 0.1), 1),  # [1.1, 1.2, ..., 2.0]\n",
    "    'n_runs_per_alpha': 30,\n",
    "    \n",
    "    # Network parameters\n",
    "    'n_cells': 50,\n",
    "    'min_flow': 1.0,\n",
    "    'barrier_height': 0.2,  # Option D from Exp 9\n",
    "    \n",
    "    # Two-phase simulation parameters\n",
    "    'cascade_duration': 200,\n",
    "    'recovery_duration': 800,\n",
    "    'dt': 0.5,\n",
    "    'cascade_sigma': 0.06,\n",
    "    'cascade_alpha': 1.5,  # Fixed Levy for cascade\n",
    "    'recovery_sigma': 0.04,  # Option D from Exp 9\n",
    "    # recovery_alpha varies in sweep\n",
    "    \n",
    "    # Seeds\n",
    "    'base_seed': 42,\n",
    "}\n",
    "\n",
    "total_sims = len(SWEEP_CONFIG['alpha_values']) * SWEEP_CONFIG['n_runs_per_alpha']\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 10: ALPHA-SWEEP CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Alpha values to test: {SWEEP_CONFIG['alpha_values']}\")\n",
    "print(f\"Runs per alpha: {SWEEP_CONFIG['n_runs_per_alpha']}\")\n",
    "print(f\"Total simulations: {total_sims}\")\n",
    "print(f\"\\nFixed parameters:\")\n",
    "print(f\"  Cascade alpha: {SWEEP_CONFIG['cascade_alpha']} (Levy)\")\n",
    "print(f\"  Cascade sigma: {SWEEP_CONFIG['cascade_sigma']}\")\n",
    "print(f\"  Recovery sigma: {SWEEP_CONFIG['recovery_sigma']}\")\n",
    "print(f\"  Barrier height: {SWEEP_CONFIG['barrier_height']}\")\n",
    "print(f\"\\nHypothesis: Critical alpha ~ 1.7 separates recovery-capable from recovery-incapable regimes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "network-header",
   "metadata": {},
   "source": [
    "## 4. Network Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "network-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sweep_network(data, config, seed=42):\n",
    "    \"\"\"\n",
    "    Create a 50-cell network for alpha sweep experiments.\n",
    "    Uses symmetric EnergyConstrainedCusp elements with Option D barrier height.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    network_matrix = data['network']\n",
    "    n_cells = config['n_cells']\n",
    "    min_flow = config['min_flow']\n",
    "    barrier_height = config['barrier_height']\n",
    "    \n",
    "    # Select top cells by total connectivity\n",
    "    total_flow = network_matrix.sum(axis=0) + network_matrix.sum(axis=1)\n",
    "    top_indices = np.argsort(total_flow)[-n_cells:]\n",
    "    \n",
    "    # Create network\n",
    "    net = EnergyConstrainedNetwork()\n",
    "    \n",
    "    # Add cusp elements\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        element = EnergyConstrainedCusp(\n",
    "            a=-1.0, b=1.0, c=0.0, x_0=0.0,\n",
    "            barrier_height=barrier_height,\n",
    "            dissipation_rate=0.1\n",
    "        )\n",
    "        net.add_element(f'cell_{i}', element)\n",
    "    \n",
    "    # Add couplings based on moisture flow\n",
    "    n_edges = 0\n",
    "    for i, idx_i in enumerate(top_indices):\n",
    "        for j, idx_j in enumerate(top_indices):\n",
    "            if i != j:\n",
    "                flow = network_matrix[idx_i, idx_j]\n",
    "                if flow > min_flow:\n",
    "                    coupling = GradientDrivenCoupling(\n",
    "                        conductivity=flow / 100.0,\n",
    "                        state_coupling=0.1\n",
    "                    )\n",
    "                    net.add_coupling(f'cell_{i}', f'cell_{j}', coupling)\n",
    "                    n_edges += 1\n",
    "    \n",
    "    return net, top_indices\n",
    "\n",
    "# Create the network (will be serialized for workers)\n",
    "network, selected_cells = create_sweep_network(amazon_data, SWEEP_CONFIG)\n",
    "print(f\"Created network: {network.n_elements} nodes, {network.number_of_edges()} edges\")\n",
    "print(f\"Barrier height: {SWEEP_CONFIG['barrier_height']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worker-header",
   "metadata": {},
   "source": [
    "## 5. Worker Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worker-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_alpha_experiment(network_bytes, recovery_alpha, config, seed):\n",
    "    \"\"\"\n",
    "    Worker function for a single alpha sweep experiment.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    network_bytes : bytes\n",
    "        Pickled network object (for Dask serialization)\n",
    "    recovery_alpha : float\n",
    "        Levy stability parameter for recovery phase\n",
    "    config : dict\n",
    "        Experiment configuration\n",
    "    seed : int\n",
    "        Random seed for this run\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Results dictionary for DataFrame aggregation\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    \n",
    "    # Add path for k3s workers\n",
    "    if '/opt/research-local/src' not in sys.path:\n",
    "        sys.path.insert(0, '/opt/research-local/src')\n",
    "    \n",
    "    from energy_constrained.solvers import run_two_phase_experiment\n",
    "    from energy_constrained.analysis import EnergyAnalyzer\n",
    "    \n",
    "    # Reconstruct network\n",
    "    network = pickle.loads(network_bytes)\n",
    "    \n",
    "    # Set seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Run two-phase experiment with this recovery alpha\n",
    "    result = run_two_phase_experiment(\n",
    "        network=network,\n",
    "        cascade_duration=config['cascade_duration'],\n",
    "        recovery_duration=config['recovery_duration'],\n",
    "        dt=config['dt'],\n",
    "        cascade_sigma=config['cascade_sigma'],\n",
    "        cascade_alpha=config['cascade_alpha'],\n",
    "        recovery_sigma=config['recovery_sigma'],\n",
    "        recovery_alpha=recovery_alpha,  # THE SWEEP VARIABLE\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Compute additional metrics\n",
    "    # Time in tipped state\n",
    "    pct_time_tipped = np.mean(result.x_full > 0) * 100\n",
    "    \n",
    "    # Count transitions (tip and recovery events)\n",
    "    n_cells = result.x_full.shape[1]\n",
    "    n_tip_events = 0\n",
    "    n_recover_events = 0\n",
    "    \n",
    "    for j in range(n_cells):\n",
    "        x_traj = result.x_full[:, j]\n",
    "        # Find crossings of x=0\n",
    "        signs = np.sign(x_traj)\n",
    "        sign_changes = np.diff(signs)\n",
    "        n_tip_events += np.sum(sign_changes > 0)  # - to + = tip\n",
    "        n_recover_events += np.sum(sign_changes < 0)  # + to - = recover\n",
    "    \n",
    "    return {\n",
    "        'recovery_alpha': recovery_alpha,\n",
    "        'seed': seed,\n",
    "        'recovery_fraction': result.metrics['recovery_fraction'],\n",
    "        'pct_tipped_cascade': result.metrics['pct_tipped_at_cascade_end'],\n",
    "        'n_permanent_tips': result.metrics['n_permanent_tips'],\n",
    "        'final_pct_tipped': result.metrics['final_pct_tipped'],\n",
    "        'pct_time_tipped': pct_time_tipped,\n",
    "        'n_tip_events': n_tip_events,\n",
    "        'n_recover_events': n_recover_events,\n",
    "        'mean_recovery_time': result.metrics['mean_recovery_time'],\n",
    "    }\n",
    "\n",
    "print(\"Worker function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-header",
   "metadata": {},
   "source": [
    "## 6. Run Alpha Sweep Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize network and SCATTER to workers (optimization)\n",
    "# This sends data ONCE to all workers instead of with every task\n",
    "network_bytes = pickle.dumps(network)\n",
    "network_future = client.scatter(network_bytes, broadcast=True)\n",
    "print(f\"Network scattered to {len(client.scheduler_info()['workers'])} workers ({len(network_bytes) / 1024:.1f} KB)\")\n",
    "\n",
    "# Submit all tasks\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPERIMENT 10: Starting Alpha-Sweep\")\n",
    "print(\"=\" * 60)\n",
    "start_time = time.time()\n",
    "\n",
    "futures = []\n",
    "task_info = []  # Track (alpha, run_idx) for each future\n",
    "\n",
    "for i, alpha in enumerate(SWEEP_CONFIG['alpha_values']):\n",
    "    for run_idx in range(SWEEP_CONFIG['n_runs_per_alpha']):\n",
    "        seed = SWEEP_CONFIG['base_seed'] + i * 1000 + run_idx\n",
    "        \n",
    "        future = client.submit(\n",
    "            run_single_alpha_experiment,\n",
    "            network_bytes=network_future,  # Use scattered reference\n",
    "            recovery_alpha=float(alpha),\n",
    "            config=SWEEP_CONFIG,\n",
    "            seed=seed,\n",
    "            key=f\"alpha_{alpha:.1f}_run_{run_idx}\",\n",
    "            pure=False  # Stochastic simulations\n",
    "        )\n",
    "        futures.append(future)\n",
    "        task_info.append((alpha, run_idx))\n",
    "\n",
    "print(f\"Submitted {len(futures)} tasks to Dask cluster\")\n",
    "print(f\"Alpha values: {list(SWEEP_CONFIG['alpha_values'])}\")\n",
    "print(f\"Runs per alpha: {SWEEP_CONFIG['n_runs_per_alpha']}\")\n",
    "\n",
    "# Collect results with progress tracking\n",
    "all_results = []\n",
    "completed_by_alpha = {float(a): 0 for a in SWEEP_CONFIG['alpha_values']}\n",
    "\n",
    "print(\"\\nProgress:\")\n",
    "for future in as_completed(futures):\n",
    "    result = future.result()\n",
    "    all_results.append(result)\n",
    "    \n",
    "    # Track progress by alpha\n",
    "    alpha = result['recovery_alpha']\n",
    "    completed_by_alpha[alpha] += 1\n",
    "    \n",
    "    # Print progress every 30 completions (one full alpha)\n",
    "    total_completed = len(all_results)\n",
    "    if total_completed % 30 == 0:\n",
    "        print(f\"  Completed {total_completed}/{len(futures)} simulations...\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nTotal runtime: {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\")\n",
    "print(f\"Average per simulation: {elapsed/len(futures):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## 7. Results Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_results)\n",
    "print(f\"Results shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "\n",
    "# Sort by alpha for consistent ordering\n",
    "df = df.sort_values(['recovery_alpha', 'seed']).reset_index(drop=True)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by alpha\n",
    "summary = df.groupby('recovery_alpha').agg({\n",
    "    'recovery_fraction': ['mean', 'std', 'min', 'max'],\n",
    "    'pct_tipped_cascade': ['mean', 'std'],\n",
    "    'n_permanent_tips': ['mean', 'std'],\n",
    "    'final_pct_tipped': ['mean', 'std'],\n",
    "    'pct_time_tipped': ['mean', 'std'],\n",
    "    'n_tip_events': ['mean', 'std'],\n",
    "    'n_recover_events': ['mean', 'std'],\n",
    "}).round(3)\n",
    "\n",
    "summary.columns = ['_'.join(col) for col in summary.columns]\n",
    "summary = summary.reset_index()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ALPHA-SWEEP SUMMARY: Recovery by Noise Stability Parameter\")\n",
    "print(\"=\" * 80)\n",
    "print(summary[['recovery_alpha', 'recovery_fraction_mean', 'recovery_fraction_std', \n",
    "               'n_permanent_tips_mean', 'n_recover_events_mean']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## 8. Primary Visualization - Transition Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-transition",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "alpha_vals = summary['recovery_alpha'].values\n",
    "\n",
    "# Panel 1: Recovery Fraction vs Alpha (KEY METRIC)\n",
    "ax = axes[0, 0]\n",
    "means = summary['recovery_fraction_mean'].values\n",
    "stds = summary['recovery_fraction_std'].values\n",
    "ax.errorbar(alpha_vals, means, yerr=stds, marker='o', capsize=4, linewidth=2, markersize=8, color='blue')\n",
    "ax.axhline(0.5, color='gray', linestyle='--', alpha=0.5, label='50% recovery')\n",
    "ax.axvline(1.7, color='red', linestyle=':', alpha=0.7, linewidth=2, label='Hypothesized critical alpha')\n",
    "ax.fill_between([1.6, 1.8], 0, 1, alpha=0.1, color='red')\n",
    "ax.set_xlabel('Recovery Alpha', fontsize=12)\n",
    "ax.set_ylabel('Recovery Fraction', fontsize=12)\n",
    "ax.set_title('Recovery Success vs Noise Stability Parameter', fontsize=14)\n",
    "ax.set_xlim(1.0, 2.1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Permanent Tips vs Alpha\n",
    "ax = axes[0, 1]\n",
    "means = summary['n_permanent_tips_mean'].values\n",
    "stds = summary['n_permanent_tips_std'].values\n",
    "ax.errorbar(alpha_vals, means, yerr=stds, marker='s', capsize=4, linewidth=2, markersize=8, color='red')\n",
    "ax.axvline(1.7, color='red', linestyle=':', alpha=0.7, linewidth=2)\n",
    "ax.set_xlabel('Recovery Alpha', fontsize=12)\n",
    "ax.set_ylabel('Permanent Tips (out of 50)', fontsize=12)\n",
    "ax.set_title('Irreversibility vs Noise Stability Parameter', fontsize=14)\n",
    "ax.set_xlim(1.0, 2.1)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 3: Recovery Events vs Alpha\n",
    "ax = axes[1, 0]\n",
    "means = summary['n_recover_events_mean'].values\n",
    "stds = summary['n_recover_events_std'].values\n",
    "ax.errorbar(alpha_vals, means, yerr=stds, marker='^', capsize=4, linewidth=2, markersize=8, color='green')\n",
    "ax.axvline(1.7, color='red', linestyle=':', alpha=0.7, linewidth=2)\n",
    "ax.set_xlabel('Recovery Alpha', fontsize=12)\n",
    "ax.set_ylabel('Number of Recovery Events', fontsize=12)\n",
    "ax.set_title('Recovery Activity vs Noise Stability Parameter', fontsize=14)\n",
    "ax.set_xlim(1.0, 2.1)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 4: % Time Tipped vs Alpha\n",
    "ax = axes[1, 1]\n",
    "means = summary['pct_time_tipped_mean'].values\n",
    "stds = summary['pct_time_tipped_std'].values\n",
    "ax.errorbar(alpha_vals, means, yerr=stds, marker='d', capsize=4, linewidth=2, markersize=8, color='purple')\n",
    "ax.axhline(50, color='gray', linestyle='--', alpha=0.5, label='50% time')\n",
    "ax.axvline(1.7, color='red', linestyle=':', alpha=0.7, linewidth=2)\n",
    "ax.set_xlabel('Recovery Alpha', fontsize=12)\n",
    "ax.set_ylabel('% Time in Tipped State', fontsize=12)\n",
    "ax.set_title('Time in Tipped State vs Noise Stability Parameter', fontsize=14)\n",
    "ax.set_xlim(1.0, 2.1)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspace/data/exp10_transition_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nPlot saved to /workspace/data/exp10_transition_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-header",
   "metadata": {},
   "source": [
    "## 9. Critical Alpha Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "find-critical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_critical_alpha(df, metric='recovery_fraction', threshold=0.1, method='interpolation'):\n",
    "    \"\"\"\n",
    "    Find alpha where metric crosses threshold.\n",
    "    \n",
    "    For recovery_fraction: finds alpha where recovery drops below threshold\n",
    "    (i.e., transition from recovery-capable to recovery-incapable)\n",
    "    \"\"\"\n",
    "    grouped = df.groupby('recovery_alpha')[metric].agg(['mean', 'std', 'count'])\n",
    "    \n",
    "    alphas = grouped.index.values\n",
    "    means = grouped['mean'].values\n",
    "    \n",
    "    # Find where mean crosses threshold (going from high to low as alpha increases)\n",
    "    for i in range(len(alphas) - 1):\n",
    "        if means[i] >= threshold and means[i+1] < threshold:\n",
    "            # Crossing between i and i+1\n",
    "            slope = (means[i+1] - means[i]) / (alphas[i+1] - alphas[i])\n",
    "            if slope != 0:\n",
    "                critical = alphas[i] + (threshold - means[i]) / slope\n",
    "            else:\n",
    "                critical = (alphas[i] + alphas[i+1]) / 2\n",
    "            \n",
    "            return {\n",
    "                'critical_alpha': critical,\n",
    "                'lower_bound': alphas[i],\n",
    "                'upper_bound': alphas[i+1],\n",
    "                'lower_value': means[i],\n",
    "                'upper_value': means[i+1],\n",
    "                'method': 'linear_interpolation',\n",
    "                'threshold': threshold\n",
    "            }\n",
    "    \n",
    "    # Check if always above or below threshold\n",
    "    if np.all(means >= threshold):\n",
    "        return {'critical_alpha': None, 'interpretation': f'All values above {threshold}'}\n",
    "    elif np.all(means < threshold):\n",
    "        return {'critical_alpha': None, 'interpretation': f'All values below {threshold}'}\n",
    "    \n",
    "    return {'critical_alpha': None, 'interpretation': 'No clear crossing found'}\n",
    "\n",
    "\n",
    "# Find critical alpha at different thresholds\n",
    "print(\"=\" * 60)\n",
    "print(\"CRITICAL ALPHA ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "thresholds = [0.30, 0.20, 0.10, 0.05]\n",
    "\n",
    "for thresh in thresholds:\n",
    "    result = find_critical_alpha(df, threshold=thresh)\n",
    "    if result['critical_alpha'] is not None:\n",
    "        print(f\"\\nThreshold = {thresh*100:.0f}% recovery:\")\n",
    "        print(f\"  Critical alpha: {result['critical_alpha']:.2f}\")\n",
    "        print(f\"  Transition region: [{result['lower_bound']:.1f}, {result['upper_bound']:.1f}]\")\n",
    "        print(f\"  Values: {result['lower_value']:.3f} -> {result['upper_value']:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\nThreshold = {thresh*100:.0f}%: {result.get('interpretation', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bootstrap-ci",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_critical_alpha(df, metric='recovery_fraction', threshold=0.1, n_bootstrap=1000):\n",
    "    \"\"\"Estimate critical alpha uncertainty via bootstrap resampling.\"\"\"\n",
    "    critical_alphas = []\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Resample within each alpha group\n",
    "        resampled_dfs = []\n",
    "        for alpha in df['recovery_alpha'].unique():\n",
    "            group = df[df['recovery_alpha'] == alpha]\n",
    "            resampled = group.sample(n=len(group), replace=True)\n",
    "            resampled_dfs.append(resampled)\n",
    "        resampled_df = pd.concat(resampled_dfs, ignore_index=True)\n",
    "        \n",
    "        result = find_critical_alpha(resampled_df, metric, threshold)\n",
    "        if result['critical_alpha'] is not None:\n",
    "            critical_alphas.append(result['critical_alpha'])\n",
    "    \n",
    "    if len(critical_alphas) > 0:\n",
    "        return {\n",
    "            'mean': np.mean(critical_alphas),\n",
    "            'std': np.std(critical_alphas),\n",
    "            'ci_95': (np.percentile(critical_alphas, 2.5), np.percentile(critical_alphas, 97.5)),\n",
    "            'n_valid': len(critical_alphas),\n",
    "            'n_bootstrap': n_bootstrap\n",
    "        }\n",
    "    else:\n",
    "        return {'mean': None, 'interpretation': 'Could not find critical alpha in bootstrap samples'}\n",
    "\n",
    "\n",
    "# Bootstrap CI for 10% threshold\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BOOTSTRAP CONFIDENCE INTERVALS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for thresh in [0.20, 0.10, 0.05]:\n",
    "    print(f\"\\nBootstrapping critical alpha for {thresh*100:.0f}% threshold...\")\n",
    "    bs_result = bootstrap_critical_alpha(df, threshold=thresh, n_bootstrap=1000)\n",
    "    \n",
    "    if bs_result['mean'] is not None:\n",
    "        print(f\"  Critical alpha: {bs_result['mean']:.3f} +/- {bs_result['std']:.3f}\")\n",
    "        print(f\"  95% CI: [{bs_result['ci_95'][0]:.3f}, {bs_result['ci_95'][1]:.3f}]\")\n",
    "        print(f\"  Valid samples: {bs_result['n_valid']}/{bs_result['n_bootstrap']}\")\n",
    "    else:\n",
    "        print(f\"  {bs_result.get('interpretation', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dist-header",
   "metadata": {},
   "source": [
    "## 10. Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-distributions",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "alpha_vals = sorted(df['recovery_alpha'].unique())\n",
    "\n",
    "# Panel 1: Box plots\n",
    "ax = axes[0]\n",
    "data_by_alpha = [df[df['recovery_alpha'] == a]['recovery_fraction'].values for a in alpha_vals]\n",
    "bp = ax.boxplot(data_by_alpha, labels=[f'{a:.1f}' for a in alpha_vals], patch_artist=True)\n",
    "\n",
    "# Color by regime\n",
    "colors = []\n",
    "for a in alpha_vals:\n",
    "    if a < 1.5:\n",
    "        colors.append('green')  # Strong Levy\n",
    "    elif a < 1.8:\n",
    "        colors.append('gold')  # Transition\n",
    "    else:\n",
    "        colors.append('red')  # Near-Gaussian\n",
    "\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "ax.axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Recovery Alpha', fontsize=12)\n",
    "ax.set_ylabel('Recovery Fraction', fontsize=12)\n",
    "ax.set_title('Recovery Fraction Distribution by Alpha', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='green', alpha=0.6, label='Levy regime (alpha < 1.5)'),\n",
    "    Patch(facecolor='gold', alpha=0.6, label='Transition (1.5 <= alpha < 1.8)'),\n",
    "    Patch(facecolor='red', alpha=0.6, label='Gaussian regime (alpha >= 1.8)')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "# Panel 2: Violin plots\n",
    "ax = axes[1]\n",
    "parts = ax.violinplot(data_by_alpha, positions=range(len(alpha_vals)),\n",
    "                      showmeans=True, showmedians=True)\n",
    "\n",
    "for i, pc in enumerate(parts['bodies']):\n",
    "    pc.set_facecolor(colors[i])\n",
    "    pc.set_alpha(0.6)\n",
    "\n",
    "ax.set_xticks(range(len(alpha_vals)))\n",
    "ax.set_xticklabels([f'{a:.1f}' for a in alpha_vals])\n",
    "ax.axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Recovery Alpha', fontsize=12)\n",
    "ax.set_ylabel('Recovery Fraction', fontsize=12)\n",
    "ax.set_title('Recovery Fraction Violin Plot by Alpha', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspace/data/exp10_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nPlot saved to /workspace/data/exp10_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regime-header",
   "metadata": {},
   "source": [
    "## 11. Regime Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classify-regimes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify each alpha into regime based on recovery fraction\n",
    "def classify_regime(recovery_fraction):\n",
    "    if recovery_fraction >= 0.30:\n",
    "        return 'Recovery-capable'\n",
    "    elif recovery_fraction >= 0.10:\n",
    "        return 'Transition'\n",
    "    else:\n",
    "        return 'Trapped'\n",
    "\n",
    "regime_table = []\n",
    "for alpha in sorted(df['recovery_alpha'].unique()):\n",
    "    subset = df[df['recovery_alpha'] == alpha]\n",
    "    mean_recovery = subset['recovery_fraction'].mean()\n",
    "    std_recovery = subset['recovery_fraction'].std()\n",
    "    mean_events = subset['n_recover_events'].mean()\n",
    "    regime = classify_regime(mean_recovery)\n",
    "    \n",
    "    regime_table.append({\n",
    "        'Alpha': alpha,\n",
    "        'Recovery Mean': f\"{mean_recovery:.3f}\",\n",
    "        'Recovery Std': f\"{std_recovery:.3f}\",\n",
    "        'Recovery Events': f\"{mean_events:.1f}\",\n",
    "        'Regime': regime\n",
    "    })\n",
    "\n",
    "regime_df = pd.DataFrame(regime_table)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"REGIME CLASSIFICATION TABLE\")\n",
    "print(\"=\" * 70)\n",
    "print(regime_df.to_string(index=False))\n",
    "\n",
    "# Identify transition region\n",
    "transition_alphas = regime_df[regime_df['Regime'] == 'Transition']['Alpha'].values\n",
    "if len(transition_alphas) > 0:\n",
    "    print(f\"\\nTransition region: alpha in [{transition_alphas.min()}, {transition_alphas.max()}]\")\n",
    "    \n",
    "# Compare to hypothesis\n",
    "print(f\"\\nHypothesis: Critical alpha ~ 1.7\")\n",
    "if len(transition_alphas) > 0:\n",
    "    mid_transition = (transition_alphas.min() + transition_alphas.max()) / 2\n",
    "    print(f\"Observed midpoint: {mid_transition:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "findings-header",
   "metadata": {},
   "source": [
    "## 12. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "findings",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 70)\nprint(\"EXPERIMENT 10: KEY FINDINGS\")\nprint(\"=\" * 70)\n\n# Find the actual critical alpha\nresult_10pct = find_critical_alpha(df, threshold=0.10)\nbs_10pct = bootstrap_critical_alpha(df, threshold=0.10, n_bootstrap=500)\n\n# Format critical alpha string safely\nif result_10pct.get('critical_alpha') is not None:\n    critical_str = f\"{result_10pct['critical_alpha']:.2f}\"\n    hypothesis_result = 'SUPPORTED' if 1.5 < result_10pct['critical_alpha'] < 1.9 else 'NEEDS REVIEW'\nelse:\n    critical_str = f\"N/A ({result_10pct.get('interpretation', 'unknown')})\"\n    hypothesis_result = 'CANNOT EVALUATE - no critical alpha found'\n\n# Format bootstrap CI string safely\nif bs_10pct.get('ci_95') is not None:\n    ci_str = f\"[{bs_10pct['ci_95'][0]:.2f}, {bs_10pct['ci_95'][1]:.2f}]\"\nelse:\n    ci_str = f\"N/A ({bs_10pct.get('interpretation', 'unknown')})\"\n\n# Format transition region strings safely\nif len(transition_alphas) > 0:\n    levy_bound = f\"{transition_alphas.min():.1f}\"\n    trans_min = f\"{transition_alphas.min():.1f}\"\n    trans_max = f\"{transition_alphas.max():.1f}\"\n    gauss_bound = f\"{transition_alphas.max():.1f}\"\n    trans_char = 'Sharp transition' if len(transition_alphas) <= 2 else 'Gradual transition'\n    trans_span = len(transition_alphas)\nelse:\n    levy_bound = trans_min = trans_max = gauss_bound = \"?\"\n    trans_char = \"Unknown\"\n    trans_span = 0\n\nprint(f\"\"\"\n1. CRITICAL ALPHA IDENTIFICATION:\n   Threshold: 10% recovery fraction\n   Critical alpha: {critical_str}\n   Bootstrap 95% CI: {ci_str}\n   \n2. HYPOTHESIS TEST:\n   Hypothesized critical alpha: 1.7\n   Observed: {hypothesis_result}\n\n3. REGIME BOUNDARIES:\n   Levy regime (recovery-capable):   alpha < {levy_bound}\n   Transition zone:                  {trans_min} <= alpha <= {trans_max}\n   Gaussian regime (trapped):        alpha > {gauss_bound}\n\n4. TRANSITION CHARACTER:\n   {trans_char} spanning {trans_span} alpha values\n\n5. RECOVERY STATISTICS BY REGIME:\n\"\"\")\n\n# Print recovery stats by regime\nfor regime in ['Recovery-capable', 'Transition', 'Trapped']:\n    alphas_in_regime = regime_df[regime_df['Regime'] == regime]['Alpha'].values\n    if len(alphas_in_regime) > 0:\n        subset = df[df['recovery_alpha'].isin(alphas_in_regime)]\n        print(f\"   {regime}:\")\n        print(f\"     Alpha range: [{alphas_in_regime.min():.1f}, {alphas_in_regime.max():.1f}]\")\n        print(f\"     Mean recovery: {subset['recovery_fraction'].mean():.3f}\")\n        print(f\"     Mean recovery events: {subset['n_recover_events'].mean():.1f}\")\n    else:\n        print(f\"   {regime}: No alpha values in this regime\")"
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## 13. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full results\n",
    "df.to_csv('/workspace/data/experiment10_alpha_sweep_results.csv', index=False)\n",
    "print(f\"Full results saved to /workspace/data/experiment10_alpha_sweep_results.csv\")\n",
    "\n",
    "# Save summary\n",
    "summary.to_csv('/workspace/data/experiment10_summary.csv', index=False)\n",
    "print(f\"Summary saved to /workspace/data/experiment10_summary.csv\")\n",
    "\n",
    "# Save regime classification\n",
    "regime_df.to_csv('/workspace/data/experiment10_regimes.csv', index=False)\n",
    "print(f\"Regime classification saved to /workspace/data/experiment10_regimes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT 10 COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "CONFIGURATION:\n",
    "- Alpha values tested: {len(SWEEP_CONFIG['alpha_values'])} ({SWEEP_CONFIG['alpha_values'].min():.1f} to {SWEEP_CONFIG['alpha_values'].max():.1f})\n",
    "- Runs per alpha: {SWEEP_CONFIG['n_runs_per_alpha']}\n",
    "- Total simulations: {len(df)}\n",
    "- Runtime: {elapsed:.1f} seconds ({elapsed/60:.1f} minutes)\n",
    "\n",
    "FILES GENERATED:\n",
    "- /workspace/data/experiment10_alpha_sweep_results.csv\n",
    "- /workspace/data/experiment10_summary.csv\n",
    "- /workspace/data/experiment10_regimes.csv\n",
    "- /workspace/data/exp10_transition_curves.png\n",
    "- /workspace/data/exp10_distributions.png\n",
    "\n",
    "NEXT STEPS:\n",
    "1. Update docs/phase4_results.md with these findings\n",
    "2. If critical alpha differs significantly from 1.7, investigate why\n",
    "3. Consider Experiment 11 (Keystone Connections) or 12 (Publication Statistics)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
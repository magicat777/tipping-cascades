{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Experiment 10b: 2D Parameter Sweep (α × σ)\n",
    "\n",
    "**Phase 4 - Mapping the Recovery-Capable Parameter Region**\n",
    "\n",
    "## Background\n",
    "\n",
    "Experiment 10 revealed that varying recovery α alone (1.1 to 2.0) produced uniformly low recovery (~1.3%) across all values. This suggests that **noise amplitude σ may be more important than noise type α** for enabling recovery.\n",
    "\n",
    "## Revised Hypothesis\n",
    "\n",
    "Recovery requires BOTH:\n",
    "1. Heavy-tailed noise (low α) for extreme jumps\n",
    "2. Sufficient noise amplitude (higher σ) to overcome the barrier\n",
    "\n",
    "## This Experiment\n",
    "\n",
    "2D sweep across recovery (α, σ) space to:\n",
    "1. Map recovery fraction as a function of both parameters\n",
    "2. Identify the recovery-capable region\n",
    "3. Determine if there's an α × σ interaction effect\n",
    "4. Find minimum intervention (σ) needed for recovery at each α\n",
    "\n",
    "| Parameter | Values |\n",
    "|-----------|--------|\n",
    "| Recovery α | [1.2, 1.4, 1.6, 1.8, 2.0] (5 values) |\n",
    "| Recovery σ | [0.04, 0.06, 0.08, 0.10, 0.12] (5 values) |\n",
    "| Grid points | 25 combinations |\n",
    "| Ensemble runs | 20 per combination |\n",
    "| **Total simulations** | 500 |\n",
    "\n",
    "## Dask Optimizations Applied\n",
    "\n",
    "Based on Experiment 10 learnings:\n",
    "- Pre-scatter network to all workers (broadcast)\n",
    "- Batch tasks by parameter combination\n",
    "- Use `client.map()` for homogeneous task submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/opt/research-local/src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from netCDF4 import Dataset\n",
    "from itertools import product\n",
    "from dask.distributed import as_completed\n",
    "\n",
    "# Core energy-constrained module\n",
    "from energy_constrained import (\n",
    "    EnergyConstrainedNetwork,\n",
    "    EnergyConstrainedCusp,\n",
    "    GradientDrivenCoupling,\n",
    "    run_two_phase_experiment,\n",
    "    get_dask_client\n",
    ")\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dask-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Dask cluster\n",
    "client = get_dask_client()\n",
    "print(f\"Connected to: {client.scheduler_info()['address']}\")\n",
    "print(f\"Workers: {len(client.scheduler_info()['workers'])}\")\n",
    "print(f\"Total threads: {sum(w['nthreads'] for w in client.scheduler_info()['workers'].values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 2. Load Amazon Moisture Recycling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/opt/research-local/data/amazon/amazon_adaptation_model/average_network/era5_new_network_data')\n",
    "\n",
    "def load_amazon_data(year=2003, months=[7, 8, 9]):\n",
    "    \"\"\"Load and average Amazon moisture recycling data for specified months.\"\"\"\n",
    "    all_rain = []\n",
    "    all_evap = []\n",
    "    all_network = []\n",
    "    \n",
    "    for month in months:\n",
    "        file_path = DATA_PATH / f'1deg_{year}_{month:02d}.nc'\n",
    "        if file_path.exists():\n",
    "            with Dataset(file_path, 'r') as ds:\n",
    "                all_rain.append(ds.variables['rain'][:])\n",
    "                all_evap.append(ds.variables['evap'][:])\n",
    "                all_network.append(ds.variables['network'][:])\n",
    "    \n",
    "    return {\n",
    "        'rain': np.mean(all_rain, axis=0),\n",
    "        'evap': np.mean(all_evap, axis=0),\n",
    "        'network': np.mean(all_network, axis=0),\n",
    "        'n_cells': len(all_rain[0])\n",
    "    }\n",
    "\n",
    "# Load 2003 (normal year) data\n",
    "amazon_data = load_amazon_data(year=2003)\n",
    "print(f\"Loaded Amazon data: {amazon_data['n_cells']} cells\")\n",
    "print(f\"Network shape: {amazon_data['network'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 3. Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 10b: 2D Alpha-Sigma Sweep Configuration\n",
    "\n",
    "SWEEP_CONFIG = {\n",
    "    # 2D sweep parameters\n",
    "    'alpha_values': np.array([1.2, 1.4, 1.6, 1.8, 2.0]),  # 5 values\n",
    "    'sigma_values': np.array([0.04, 0.06, 0.08, 0.10, 0.12]),  # 5 values\n",
    "    'n_runs_per_combo': 20,\n",
    "    \n",
    "    # Network parameters\n",
    "    'n_cells': 50,\n",
    "    'min_flow': 1.0,\n",
    "    'barrier_height': 0.2,  # Option D from Exp 9\n",
    "    \n",
    "    # Two-phase simulation parameters\n",
    "    'cascade_duration': 200,\n",
    "    'recovery_duration': 800,\n",
    "    'dt': 0.5,\n",
    "    'cascade_sigma': 0.06,\n",
    "    'cascade_alpha': 1.5,  # Fixed Lévy for cascade\n",
    "    \n",
    "    # Seeds\n",
    "    'base_seed': 42,\n",
    "}\n",
    "\n",
    "# Calculate grid\n",
    "n_alpha = len(SWEEP_CONFIG['alpha_values'])\n",
    "n_sigma = len(SWEEP_CONFIG['sigma_values'])\n",
    "n_combos = n_alpha * n_sigma\n",
    "total_sims = n_combos * SWEEP_CONFIG['n_runs_per_combo']\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 10b: 2D ALPHA-SIGMA SWEEP CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Alpha values: {SWEEP_CONFIG['alpha_values']}\")\n",
    "print(f\"Sigma values: {SWEEP_CONFIG['sigma_values']}\")\n",
    "print(f\"Grid size: {n_alpha} x {n_sigma} = {n_combos} combinations\")\n",
    "print(f\"Runs per combination: {SWEEP_CONFIG['n_runs_per_combo']}\")\n",
    "print(f\"Total simulations: {total_sims}\")\n",
    "print(f\"\\nEstimated runtime: ~{total_sims * 67 / 300 / 60:.1f} hours (based on Exp 10)\")\n",
    "print(f\"\\nFixed parameters:\")\n",
    "print(f\"  Cascade alpha: {SWEEP_CONFIG['cascade_alpha']}\")\n",
    "print(f\"  Cascade sigma: {SWEEP_CONFIG['cascade_sigma']}\")\n",
    "print(f\"  Barrier height: {SWEEP_CONFIG['barrier_height']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "network-header",
   "metadata": {},
   "source": [
    "## 4. Network Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "network-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sweep_network(data, config, seed=42):\n",
    "    \"\"\"\n",
    "    Create a 50-cell network for sweep experiments.\n",
    "    Uses symmetric EnergyConstrainedCusp elements.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    network_matrix = data['network']\n",
    "    n_cells = config['n_cells']\n",
    "    min_flow = config['min_flow']\n",
    "    barrier_height = config['barrier_height']\n",
    "    \n",
    "    # Select top cells by total connectivity\n",
    "    total_flow = network_matrix.sum(axis=0) + network_matrix.sum(axis=1)\n",
    "    top_indices = np.argsort(total_flow)[-n_cells:]\n",
    "    \n",
    "    # Create network\n",
    "    net = EnergyConstrainedNetwork()\n",
    "    \n",
    "    # Add cusp elements\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        element = EnergyConstrainedCusp(\n",
    "            a=-1.0, b=1.0, c=0.0, x_0=0.0,\n",
    "            barrier_height=barrier_height,\n",
    "            dissipation_rate=0.1\n",
    "        )\n",
    "        net.add_element(f'cell_{i}', element)\n",
    "    \n",
    "    # Add couplings based on moisture flow\n",
    "    n_edges = 0\n",
    "    for i, idx_i in enumerate(top_indices):\n",
    "        for j, idx_j in enumerate(top_indices):\n",
    "            if i != j:\n",
    "                flow = network_matrix[idx_i, idx_j]\n",
    "                if flow > min_flow:\n",
    "                    coupling = GradientDrivenCoupling(\n",
    "                        conductivity=flow / 100.0,\n",
    "                        state_coupling=0.1\n",
    "                    )\n",
    "                    net.add_coupling(f'cell_{i}', f'cell_{j}', coupling)\n",
    "                    n_edges += 1\n",
    "    \n",
    "    return net, top_indices\n",
    "\n",
    "# Create the network\n",
    "network, selected_cells = create_sweep_network(amazon_data, SWEEP_CONFIG)\n",
    "print(f\"Created network: {network.n_elements} nodes, {network.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worker-header",
   "metadata": {},
   "source": [
    "## 5. Worker Function (Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worker-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(args):\n",
    "    \"\"\"\n",
    "    Worker function for a single (alpha, sigma) experiment.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    args : tuple\n",
    "        (network_bytes, recovery_alpha, recovery_sigma, config, seed)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Results dictionary for DataFrame aggregation\n",
    "    \"\"\"\n",
    "    network_bytes, recovery_alpha, recovery_sigma, config, seed = args\n",
    "    \n",
    "    import sys\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    \n",
    "    # Add path for k3s workers\n",
    "    if '/opt/research-local/src' not in sys.path:\n",
    "        sys.path.insert(0, '/opt/research-local/src')\n",
    "    \n",
    "    from energy_constrained.solvers import run_two_phase_experiment\n",
    "    \n",
    "    # Reconstruct network\n",
    "    network = pickle.loads(network_bytes)\n",
    "    \n",
    "    # Set seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Run two-phase experiment\n",
    "    result = run_two_phase_experiment(\n",
    "        network=network,\n",
    "        cascade_duration=config['cascade_duration'],\n",
    "        recovery_duration=config['recovery_duration'],\n",
    "        dt=config['dt'],\n",
    "        cascade_sigma=config['cascade_sigma'],\n",
    "        cascade_alpha=config['cascade_alpha'],\n",
    "        recovery_sigma=recovery_sigma,  # SWEEP VARIABLE\n",
    "        recovery_alpha=recovery_alpha,  # SWEEP VARIABLE\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Compute additional metrics\n",
    "    pct_time_tipped = np.mean(result.x_full > 0) * 100\n",
    "    \n",
    "    # Count transitions\n",
    "    n_cells = result.x_full.shape[1]\n",
    "    n_tip_events = 0\n",
    "    n_recover_events = 0\n",
    "    \n",
    "    for j in range(n_cells):\n",
    "        x_traj = result.x_full[:, j]\n",
    "        signs = np.sign(x_traj)\n",
    "        sign_changes = np.diff(signs)\n",
    "        n_tip_events += np.sum(sign_changes > 0)\n",
    "        n_recover_events += np.sum(sign_changes < 0)\n",
    "    \n",
    "    return {\n",
    "        'recovery_alpha': recovery_alpha,\n",
    "        'recovery_sigma': recovery_sigma,\n",
    "        'seed': seed,\n",
    "        'recovery_fraction': result.metrics['recovery_fraction'],\n",
    "        'pct_tipped_cascade': result.metrics['pct_tipped_at_cascade_end'],\n",
    "        'n_permanent_tips': result.metrics['n_permanent_tips'],\n",
    "        'final_pct_tipped': result.metrics['final_pct_tipped'],\n",
    "        'pct_time_tipped': pct_time_tipped,\n",
    "        'n_tip_events': n_tip_events,\n",
    "        'n_recover_events': n_recover_events,\n",
    "        'mean_recovery_time': result.metrics['mean_recovery_time'],\n",
    "    }\n",
    "\n",
    "print(\"Worker function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-header",
   "metadata": {},
   "source": [
    "## 6. Run 2D Sweep Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize network once\n",
    "network_bytes = pickle.dumps(network)\n",
    "print(f\"Network serialized: {len(network_bytes) / 1024:.1f} KB\")\n",
    "\n",
    "# Pre-scatter network to all workers for efficiency\n",
    "network_future = client.scatter(network_bytes, broadcast=True)\n",
    "print(\"Network broadcast to all workers\")\n",
    "\n",
    "# Build task arguments\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPERIMENT 10b: Starting 2D Alpha-Sigma Sweep\")\n",
    "print(\"=\" * 60)\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate all (alpha, sigma, run) combinations\n",
    "task_args = []\n",
    "combo_idx = 0\n",
    "for alpha in SWEEP_CONFIG['alpha_values']:\n",
    "    for sigma in SWEEP_CONFIG['sigma_values']:\n",
    "        for run_idx in range(SWEEP_CONFIG['n_runs_per_combo']):\n",
    "            seed = SWEEP_CONFIG['base_seed'] + combo_idx * 1000 + run_idx\n",
    "            task_args.append((network_bytes, float(alpha), float(sigma), SWEEP_CONFIG, seed))\n",
    "        combo_idx += 1\n",
    "\n",
    "print(f\"Generated {len(task_args)} task arguments\")\n",
    "print(f\"Parameter grid: {n_alpha} alphas x {n_sigma} sigmas x {SWEEP_CONFIG['n_runs_per_combo']} runs\")\n",
    "\n",
    "# Submit using client.map for efficient batching\n",
    "futures = client.map(run_single_experiment, task_args)\n",
    "print(f\"Submitted {len(futures)} tasks via client.map()\")\n",
    "\n",
    "# Collect results with progress tracking\n",
    "all_results = []\n",
    "print(\"\\nProgress:\")\n",
    "\n",
    "for i, future in enumerate(as_completed(futures)):\n",
    "    result = future.result()\n",
    "    all_results.append(result)\n",
    "    \n",
    "    # Print progress every 50 completions\n",
    "    if (i + 1) % 50 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = (i + 1) / elapsed\n",
    "        remaining = (len(futures) - i - 1) / rate if rate > 0 else 0\n",
    "        print(f\"  Completed {i+1}/{len(futures)} ({100*(i+1)/len(futures):.1f}%) - \"\n",
    "              f\"Elapsed: {elapsed/60:.1f}min, ETA: {remaining/60:.1f}min\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"COMPLETE: {len(all_results)} simulations in {elapsed:.1f}s ({elapsed/60:.1f} min)\")\n",
    "print(f\"Average per simulation: {elapsed/len(all_results):.2f} seconds\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## 7. Results Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_results)\n",
    "print(f\"Results shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Sort for consistent ordering\n",
    "df = df.sort_values(['recovery_alpha', 'recovery_sigma', 'seed']).reset_index(drop=True)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by (alpha, sigma) combination\n",
    "summary = df.groupby(['recovery_alpha', 'recovery_sigma']).agg({\n",
    "    'recovery_fraction': ['mean', 'std'],\n",
    "    'n_permanent_tips': ['mean', 'std'],\n",
    "    'final_pct_tipped': ['mean', 'std'],\n",
    "    'n_recover_events': ['mean', 'std'],\n",
    "}).round(4)\n",
    "\n",
    "summary.columns = ['_'.join(col) for col in summary.columns]\n",
    "summary = summary.reset_index()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"2D SWEEP SUMMARY: Recovery by (Alpha, Sigma)\")\n",
    "print(\"=\" * 80)\n",
    "print(summary[['recovery_alpha', 'recovery_sigma', 'recovery_fraction_mean', \n",
    "               'recovery_fraction_std', 'n_permanent_tips_mean']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heatmap-header",
   "metadata": {},
   "source": [
    "## 8. Primary Visualization - Recovery Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table for heatmap\n",
    "pivot_recovery = summary.pivot(index='recovery_alpha', columns='recovery_sigma', \n",
    "                                values='recovery_fraction_mean')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Panel 1: Recovery Fraction Heatmap\n",
    "ax = axes[0]\n",
    "im = ax.imshow(pivot_recovery.values, cmap='RdYlGn', aspect='auto', \n",
    "               vmin=0, vmax=max(0.5, pivot_recovery.values.max()))\n",
    "ax.set_xticks(range(len(SWEEP_CONFIG['sigma_values'])))\n",
    "ax.set_xticklabels([f'{s:.2f}' for s in SWEEP_CONFIG['sigma_values']])\n",
    "ax.set_yticks(range(len(SWEEP_CONFIG['alpha_values'])))\n",
    "ax.set_yticklabels([f'{a:.1f}' for a in SWEEP_CONFIG['alpha_values']])\n",
    "ax.set_xlabel('Recovery σ (noise amplitude)', fontsize=12)\n",
    "ax.set_ylabel('Recovery α (Lévy stability)', fontsize=12)\n",
    "ax.set_title('Recovery Fraction by (α, σ)', fontsize=14)\n",
    "\n",
    "# Add values to cells\n",
    "for i in range(len(SWEEP_CONFIG['alpha_values'])):\n",
    "    for j in range(len(SWEEP_CONFIG['sigma_values'])):\n",
    "        val = pivot_recovery.values[i, j]\n",
    "        color = 'white' if val < 0.25 else 'black'\n",
    "        ax.text(j, i, f'{val:.2f}', ha='center', va='center', color=color, fontsize=10)\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Recovery Fraction')\n",
    "\n",
    "# Panel 2: Permanent Tips Heatmap\n",
    "pivot_tips = summary.pivot(index='recovery_alpha', columns='recovery_sigma',\n",
    "                           values='n_permanent_tips_mean')\n",
    "\n",
    "ax = axes[1]\n",
    "im2 = ax.imshow(pivot_tips.values, cmap='YlOrRd', aspect='auto')\n",
    "ax.set_xticks(range(len(SWEEP_CONFIG['sigma_values'])))\n",
    "ax.set_xticklabels([f'{s:.2f}' for s in SWEEP_CONFIG['sigma_values']])\n",
    "ax.set_yticks(range(len(SWEEP_CONFIG['alpha_values'])))\n",
    "ax.set_yticklabels([f'{a:.1f}' for a in SWEEP_CONFIG['alpha_values']])\n",
    "ax.set_xlabel('Recovery σ (noise amplitude)', fontsize=12)\n",
    "ax.set_ylabel('Recovery α (Lévy stability)', fontsize=12)\n",
    "ax.set_title('Permanent Tips by (α, σ)', fontsize=14)\n",
    "\n",
    "# Add values to cells\n",
    "for i in range(len(SWEEP_CONFIG['alpha_values'])):\n",
    "    for j in range(len(SWEEP_CONFIG['sigma_values'])):\n",
    "        val = pivot_tips.values[i, j]\n",
    "        color = 'white' if val > 30 else 'black'\n",
    "        ax.text(j, i, f'{val:.1f}', ha='center', va='center', color=color, fontsize=10)\n",
    "\n",
    "plt.colorbar(im2, ax=ax, label='Permanent Tips (out of 50)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspace/data/exp10b_heatmaps.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nPlot saved to /workspace/data/exp10b_heatmaps.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "line-header",
   "metadata": {},
   "source": [
    "## 9. Line Plots - Recovery vs σ at Each α"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-lines",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(SWEEP_CONFIG['alpha_values'])))\n",
    "\n",
    "# Panel 1: Recovery vs Sigma, lines for each Alpha\n",
    "ax = axes[0]\n",
    "for i, alpha in enumerate(SWEEP_CONFIG['alpha_values']):\n",
    "    subset = summary[summary['recovery_alpha'] == alpha]\n",
    "    ax.errorbar(subset['recovery_sigma'], subset['recovery_fraction_mean'],\n",
    "                yerr=subset['recovery_fraction_std'], marker='o', capsize=3,\n",
    "                label=f'α={alpha:.1f}', color=colors[i], linewidth=2, markersize=8)\n",
    "\n",
    "ax.axhline(0.1, color='red', linestyle='--', alpha=0.5, label='10% threshold')\n",
    "ax.axhline(0.3, color='orange', linestyle='--', alpha=0.5, label='30% threshold')\n",
    "ax.set_xlabel('Recovery σ', fontsize=12)\n",
    "ax.set_ylabel('Recovery Fraction', fontsize=12)\n",
    "ax.set_title('Recovery vs Noise Amplitude by α', fontsize=14)\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, max(0.6, summary['recovery_fraction_mean'].max() + 0.1))\n",
    "\n",
    "# Panel 2: Recovery vs Alpha, lines for each Sigma\n",
    "ax = axes[1]\n",
    "colors2 = plt.cm.plasma(np.linspace(0, 1, len(SWEEP_CONFIG['sigma_values'])))\n",
    "for i, sigma in enumerate(SWEEP_CONFIG['sigma_values']):\n",
    "    subset = summary[summary['recovery_sigma'] == sigma]\n",
    "    ax.errorbar(subset['recovery_alpha'], subset['recovery_fraction_mean'],\n",
    "                yerr=subset['recovery_fraction_std'], marker='s', capsize=3,\n",
    "                label=f'σ={sigma:.2f}', color=colors2[i], linewidth=2, markersize=8)\n",
    "\n",
    "ax.axhline(0.1, color='red', linestyle='--', alpha=0.5, label='10% threshold')\n",
    "ax.axhline(0.3, color='orange', linestyle='--', alpha=0.5, label='30% threshold')\n",
    "ax.set_xlabel('Recovery α', fontsize=12)\n",
    "ax.set_ylabel('Recovery Fraction', fontsize=12)\n",
    "ax.set_title('Recovery vs Lévy Stability by σ', fontsize=14)\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, max(0.6, summary['recovery_fraction_mean'].max() + 0.1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspace/data/exp10b_lines.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nPlot saved to /workspace/data/exp10b_lines.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threshold-header",
   "metadata": {},
   "source": [
    "## 10. Critical Threshold Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "find-thresholds",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sigma_threshold(summary_df, alpha, recovery_threshold=0.1):\n",
    "    \"\"\"\n",
    "    Find minimum sigma needed to achieve recovery_threshold at given alpha.\n",
    "    Uses linear interpolation between grid points.\n",
    "    \"\"\"\n",
    "    subset = summary_df[summary_df['recovery_alpha'] == alpha].sort_values('recovery_sigma')\n",
    "    sigmas = subset['recovery_sigma'].values\n",
    "    recoveries = subset['recovery_fraction_mean'].values\n",
    "    \n",
    "    # Find crossing point\n",
    "    for i in range(len(sigmas) - 1):\n",
    "        if recoveries[i] < recovery_threshold <= recoveries[i+1]:\n",
    "            # Linear interpolation\n",
    "            slope = (recoveries[i+1] - recoveries[i]) / (sigmas[i+1] - sigmas[i])\n",
    "            if slope > 0:\n",
    "                sigma_critical = sigmas[i] + (recovery_threshold - recoveries[i]) / slope\n",
    "                return sigma_critical\n",
    "    \n",
    "    # Check if always above or below\n",
    "    if recoveries.max() < recovery_threshold:\n",
    "        return None  # Never reaches threshold\n",
    "    elif recoveries.min() >= recovery_threshold:\n",
    "        return sigmas.min()  # Always above threshold\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CRITICAL SIGMA THRESHOLDS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "threshold_results = []\n",
    "for recovery_thresh in [0.10, 0.20, 0.30]:\n",
    "    print(f\"\\nRecovery threshold: {recovery_thresh*100:.0f}%\")\n",
    "    for alpha in SWEEP_CONFIG['alpha_values']:\n",
    "        sigma_crit = find_sigma_threshold(summary, alpha, recovery_thresh)\n",
    "        if sigma_crit is not None:\n",
    "            print(f\"  α={alpha:.1f}: σ_critical = {sigma_crit:.3f}\")\n",
    "            threshold_results.append({\n",
    "                'recovery_threshold': recovery_thresh,\n",
    "                'alpha': alpha,\n",
    "                'sigma_critical': sigma_crit\n",
    "            })\n",
    "        else:\n",
    "            print(f\"  α={alpha:.1f}: threshold not reached in σ range\")\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "if len(threshold_df) > 0:\n",
    "    print(\"\\nCritical thresholds found:\")\n",
    "    print(threshold_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regime-header",
   "metadata": {},
   "source": [
    "## 11. Regime Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classify-regimes",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_regime(recovery_fraction):\n",
    "    if recovery_fraction >= 0.30:\n",
    "        return 'Recovery-capable'\n",
    "    elif recovery_fraction >= 0.10:\n",
    "        return 'Transition'\n",
    "    else:\n",
    "        return 'Trapped'\n",
    "\n",
    "summary['regime'] = summary['recovery_fraction_mean'].apply(classify_regime)\n",
    "\n",
    "# Create regime heatmap\n",
    "regime_map = {'Trapped': 0, 'Transition': 1, 'Recovery-capable': 2}\n",
    "summary['regime_code'] = summary['regime'].map(regime_map)\n",
    "\n",
    "pivot_regime = summary.pivot(index='recovery_alpha', columns='recovery_sigma',\n",
    "                              values='regime_code')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Custom colormap: red (trapped) -> yellow (transition) -> green (recovery)\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap(['#d73027', '#fee08b', '#1a9850'])\n",
    "\n",
    "im = ax.imshow(pivot_regime.values, cmap=cmap, aspect='auto', vmin=0, vmax=2)\n",
    "ax.set_xticks(range(len(SWEEP_CONFIG['sigma_values'])))\n",
    "ax.set_xticklabels([f'{s:.2f}' for s in SWEEP_CONFIG['sigma_values']])\n",
    "ax.set_yticks(range(len(SWEEP_CONFIG['alpha_values'])))\n",
    "ax.set_yticklabels([f'{a:.1f}' for a in SWEEP_CONFIG['alpha_values']])\n",
    "ax.set_xlabel('Recovery σ (noise amplitude)', fontsize=12)\n",
    "ax.set_ylabel('Recovery α (Lévy stability)', fontsize=12)\n",
    "ax.set_title('Regime Classification Map', fontsize=14)\n",
    "\n",
    "# Add regime labels to cells\n",
    "regime_labels = {0: 'T', 1: 'TR', 2: 'R'}\n",
    "for i in range(len(SWEEP_CONFIG['alpha_values'])):\n",
    "    for j in range(len(SWEEP_CONFIG['sigma_values'])):\n",
    "        val = pivot_regime.values[i, j]\n",
    "        label = regime_labels.get(val, '?')\n",
    "        color = 'white' if val == 0 else 'black'\n",
    "        ax.text(j, i, label, ha='center', va='center', color=color, fontsize=12, fontweight='bold')\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#d73027', label='Trapped (T): <10% recovery'),\n",
    "    Patch(facecolor='#fee08b', label='Transition (TR): 10-30% recovery'),\n",
    "    Patch(facecolor='#1a9850', label='Recovery-capable (R): >30% recovery')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspace/data/exp10b_regime_map.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nPlot saved to /workspace/data/exp10b_regime_map.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "findings-header",
   "metadata": {},
   "source": [
    "## 12. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT 10b: KEY FINDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Count regimes\n",
    "regime_counts = summary['regime'].value_counts()\n",
    "\n",
    "# Find best and worst combinations\n",
    "best_combo = summary.loc[summary['recovery_fraction_mean'].idxmax()]\n",
    "worst_combo = summary.loc[summary['recovery_fraction_mean'].idxmin()]\n",
    "\n",
    "print(f\"\"\"\n",
    "1. PARAMETER SPACE COVERAGE:\n",
    "   Grid: {n_alpha} α values × {n_sigma} σ values = {n_combos} combinations\n",
    "   Runs per combination: {SWEEP_CONFIG['n_runs_per_combo']}\n",
    "   Total simulations: {len(df)}\n",
    "\n",
    "2. REGIME DISTRIBUTION:\n",
    "   Trapped (<10% recovery):        {regime_counts.get('Trapped', 0)} combinations\n",
    "   Transition (10-30% recovery):   {regime_counts.get('Transition', 0)} combinations  \n",
    "   Recovery-capable (>30%):        {regime_counts.get('Recovery-capable', 0)} combinations\n",
    "\n",
    "3. BEST RECOVERY:\n",
    "   α={best_combo['recovery_alpha']:.1f}, σ={best_combo['recovery_sigma']:.2f}\n",
    "   Recovery fraction: {best_combo['recovery_fraction_mean']:.3f} ± {best_combo['recovery_fraction_std']:.3f}\n",
    "\n",
    "4. WORST RECOVERY:\n",
    "   α={worst_combo['recovery_alpha']:.1f}, σ={worst_combo['recovery_sigma']:.2f}\n",
    "   Recovery fraction: {worst_combo['recovery_fraction_mean']:.3f} ± {worst_combo['recovery_fraction_std']:.3f}\n",
    "\n",
    "5. KEY INSIGHTS:\n",
    "\"\"\")\n",
    "\n",
    "# Analyze which parameter matters more\n",
    "alpha_effect = summary.groupby('recovery_alpha')['recovery_fraction_mean'].mean()\n",
    "sigma_effect = summary.groupby('recovery_sigma')['recovery_fraction_mean'].mean()\n",
    "\n",
    "alpha_range = alpha_effect.max() - alpha_effect.min()\n",
    "sigma_range = sigma_effect.max() - sigma_effect.min()\n",
    "\n",
    "print(f\"   Effect of α (averaging over σ): range = {alpha_range:.3f}\")\n",
    "print(f\"   Effect of σ (averaging over α): range = {sigma_range:.3f}\")\n",
    "\n",
    "if sigma_range > alpha_range * 1.5:\n",
    "    print(\"   → σ (noise amplitude) has STRONGER effect than α (noise type)\")\n",
    "elif alpha_range > sigma_range * 1.5:\n",
    "    print(\"   → α (noise type) has STRONGER effect than σ (noise amplitude)\")\n",
    "else:\n",
    "    print(\"   → Both parameters have comparable effects\")\n",
    "\n",
    "# Check for interaction\n",
    "print(f\"\\n6. α × σ INTERACTION:\")\n",
    "for alpha in [SWEEP_CONFIG['alpha_values'].min(), SWEEP_CONFIG['alpha_values'].max()]:\n",
    "    subset = summary[summary['recovery_alpha'] == alpha]\n",
    "    slope = (subset['recovery_fraction_mean'].iloc[-1] - subset['recovery_fraction_mean'].iloc[0]) / \\\n",
    "            (subset['recovery_sigma'].iloc[-1] - subset['recovery_sigma'].iloc[0])\n",
    "    print(f\"   At α={alpha:.1f}: d(recovery)/d(σ) ≈ {slope:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## 13. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full results\n",
    "df.to_csv('/workspace/data/experiment10b_alpha_sigma_results.csv', index=False)\n",
    "print(f\"Full results saved to /workspace/data/experiment10b_alpha_sigma_results.csv\")\n",
    "\n",
    "# Save summary\n",
    "summary.to_csv('/workspace/data/experiment10b_summary.csv', index=False)\n",
    "print(f\"Summary saved to /workspace/data/experiment10b_summary.csv\")\n",
    "\n",
    "# Save threshold analysis\n",
    "if len(threshold_df) > 0:\n",
    "    threshold_df.to_csv('/workspace/data/experiment10b_thresholds.csv', index=False)\n",
    "    print(f\"Thresholds saved to /workspace/data/experiment10b_thresholds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT 10b COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "CONFIGURATION:\n",
    "- Alpha values: {list(SWEEP_CONFIG['alpha_values'])}\n",
    "- Sigma values: {list(SWEEP_CONFIG['sigma_values'])}\n",
    "- Grid: {n_combos} combinations\n",
    "- Runs per combo: {SWEEP_CONFIG['n_runs_per_combo']}\n",
    "- Total simulations: {len(df)}\n",
    "- Runtime: {elapsed:.1f}s ({elapsed/60:.1f} min)\n",
    "\n",
    "FILES GENERATED:\n",
    "- /workspace/data/experiment10b_alpha_sigma_results.csv\n",
    "- /workspace/data/experiment10b_summary.csv\n",
    "- /workspace/data/experiment10b_thresholds.csv\n",
    "- /workspace/data/exp10b_heatmaps.png\n",
    "- /workspace/data/exp10b_lines.png\n",
    "- /workspace/data/exp10b_regime_map.png\n",
    "\n",
    "NEXT STEPS:\n",
    "1. Update docs/phase4_results.md with findings\n",
    "2. If recovery region found, proceed to Experiment 11 (Keystone Connections)\n",
    "3. If still no recovery, consider reducing barrier height further\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

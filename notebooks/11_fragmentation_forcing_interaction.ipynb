{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Experiment 11: Fragmentation-Forcing Interaction\n",
    "\n",
    "**Phase 4 - Combining Key Findings from Experiments 8 and 10c**\n",
    "\n",
    "## Background\n",
    "\n",
    "Two validated findings from Phase 4:\n",
    "\n",
    "1. **Experiment 8**: Network fragmentation creates asymmetric dynamics\n",
    "   - At 10% edge retention, tip/recovery ratio = 1.148 (14.8% asymmetry)\n",
    "   - Fragmentation reduces thermodynamic activity by 96.5%\n",
    "\n",
    "2. **Experiment 10c**: Active forcing enables recovery\n",
    "   - Passive recovery: 38.6%\n",
    "   - Linear relationship: recovery ≈ 0.74 × |f| + 0.51\n",
    "   - 50% recovery requires |f| ≥ 0.10\n",
    "\n",
    "## Key Question\n",
    "\n",
    "**Does fragmentation increase the forcing required for recovery?**\n",
    "\n",
    "If fragmented networks are harder to recover (higher asymmetry), we expect:\n",
    "- Steeper forcing-recovery curves at lower retention\n",
    "- Higher forcing thresholds for 50% recovery\n",
    "- Possible non-linear interactions\n",
    "\n",
    "## Experimental Design\n",
    "\n",
    "| Parameter | Values |\n",
    "|-----------|--------|\n",
    "| Edge retention | [1.0, 0.5, 0.25, 0.10] |\n",
    "| Forcing strength | [0.0, -0.1, -0.2, -0.3, -0.5] |\n",
    "| Ensemble runs | 20 per condition |\n",
    "| **Total simulations** | 4 × 5 × 20 = **400** |\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "At 10% retention, the forcing required for 50% recovery will be **2-3× higher** than the intact network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/opt/research-local/src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from netCDF4 import Dataset\n",
    "from dask.distributed import as_completed\n",
    "\n",
    "# Core energy-constrained module\n",
    "from energy_constrained import (\n",
    "    EnergyConstrainedNetwork,\n",
    "    EnergyConstrainedCusp,\n",
    "    GradientDrivenCoupling,\n",
    "    run_two_phase_experiment,\n",
    "    get_dask_client,\n",
    "    fragment_network,\n",
    "    compute_network_metrics\n",
    ")\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dask-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Dask cluster\n",
    "client = get_dask_client()\n",
    "print(f\"Connected to: {client.scheduler_info()['address']}\")\n",
    "print(f\"Workers: {len(client.scheduler_info()['workers'])}\")\n",
    "print(f\"Total threads: {sum(w['nthreads'] for w in client.scheduler_info()['workers'].values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 2. Load Amazon Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/opt/research-local/data/amazon/amazon_adaptation_model/average_network/era5_new_network_data')\n",
    "\n",
    "def load_amazon_data(year=2003, months=[7, 8, 9]):\n",
    "    \"\"\"Load and average Amazon moisture recycling data.\"\"\"\n",
    "    all_rain = []\n",
    "    all_evap = []\n",
    "    all_network = []\n",
    "    \n",
    "    for month in months:\n",
    "        file_path = DATA_PATH / f'1deg_{year}_{month:02d}.nc'\n",
    "        if file_path.exists():\n",
    "            with Dataset(file_path, 'r') as ds:\n",
    "                all_rain.append(ds.variables['rain'][:])\n",
    "                all_evap.append(ds.variables['evap'][:])\n",
    "                all_network.append(ds.variables['network'][:])\n",
    "    \n",
    "    return {\n",
    "        'rain': np.mean(all_rain, axis=0),\n",
    "        'evap': np.mean(all_evap, axis=0),\n",
    "        'network': np.mean(all_network, axis=0),\n",
    "        'n_cells': len(all_rain[0])\n",
    "    }\n",
    "\n",
    "amazon_data = load_amazon_data(year=2003)\n",
    "print(f\"Loaded Amazon data: {amazon_data['n_cells']} cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 3. Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 11: Fragmentation × Forcing Interaction\n",
    "\n",
    "SWEEP_CONFIG = {\n",
    "    # Two-dimensional sweep\n",
    "    'retention_values': [1.0, 0.5, 0.25, 0.10],\n",
    "    'forcing_values': np.array([0.0, -0.10, -0.20, -0.30, -0.50]),\n",
    "    'n_runs_per_condition': 20,\n",
    "    \n",
    "    # Network parameters\n",
    "    'n_cells': 50,\n",
    "    'min_flow': 1.0,\n",
    "    'barrier_height': 0.2,\n",
    "    \n",
    "    # Two-phase simulation parameters\n",
    "    'cascade_duration': 200,\n",
    "    'recovery_duration': 800,\n",
    "    'dt': 0.5,\n",
    "    'cascade_sigma': 0.06,\n",
    "    'cascade_alpha': 1.5,\n",
    "    'recovery_sigma': 0.04,\n",
    "    'recovery_alpha': 2.0,  # Gaussian\n",
    "    \n",
    "    # Fragmentation method\n",
    "    'fragmentation_method': 'random',\n",
    "    \n",
    "    # Seeds\n",
    "    'base_seed': 42,\n",
    "}\n",
    "\n",
    "n_retention = len(SWEEP_CONFIG['retention_values'])\n",
    "n_forcing = len(SWEEP_CONFIG['forcing_values'])\n",
    "n_runs = SWEEP_CONFIG['n_runs_per_condition']\n",
    "total_sims = n_retention * n_forcing * n_runs\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 11: FRAGMENTATION × FORCING INTERACTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Retention levels: {SWEEP_CONFIG['retention_values']}\")\n",
    "print(f\"Forcing values: {list(SWEEP_CONFIG['forcing_values'])}\")\n",
    "print(f\"Runs per condition: {n_runs}\")\n",
    "print(f\"Total simulations: {total_sims}\")\n",
    "print(f\"\\nEstimated runtime: ~{total_sims * 2 / 60:.0f} minutes on 14 workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "network-header",
   "metadata": {},
   "source": [
    "## 4. Network Creation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "network-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_amazon_network(data, config, seed=42):\n",
    "    \"\"\"Create 50-cell Amazon network.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    network_matrix = data['network']\n",
    "    n_cells = config['n_cells']\n",
    "    min_flow = config['min_flow']\n",
    "    barrier_height = config['barrier_height']\n",
    "    \n",
    "    total_flow = network_matrix.sum(axis=0) + network_matrix.sum(axis=1)\n",
    "    top_indices = np.argsort(total_flow)[-n_cells:]\n",
    "    \n",
    "    net = EnergyConstrainedNetwork()\n",
    "    \n",
    "    for i, idx in enumerate(top_indices):\n",
    "        element = EnergyConstrainedCusp(\n",
    "            a=-1.0, b=1.0, c=0.0, x_0=0.0,\n",
    "            barrier_height=barrier_height,\n",
    "            dissipation_rate=0.1\n",
    "        )\n",
    "        net.add_element(f'cell_{i}', element)\n",
    "    \n",
    "    n_edges = 0\n",
    "    for i, idx_i in enumerate(top_indices):\n",
    "        for j, idx_j in enumerate(top_indices):\n",
    "            if i != j:\n",
    "                flow = network_matrix[idx_i, idx_j]\n",
    "                if flow > min_flow:\n",
    "                    coupling = GradientDrivenCoupling(\n",
    "                        conductivity=flow / 100.0,\n",
    "                        state_coupling=0.1\n",
    "                    )\n",
    "                    net.add_coupling(f'cell_{i}', f'cell_{j}', coupling)\n",
    "                    n_edges += 1\n",
    "    \n",
    "    return net, top_indices\n",
    "\n",
    "# Create baseline network\n",
    "baseline_network, selected_cells = create_amazon_network(amazon_data, SWEEP_CONFIG)\n",
    "print(f\"Baseline network: {baseline_network.n_elements} nodes, {baseline_network.number_of_edges()} edges\")\n",
    "\n",
    "# Test fragmentation\n",
    "print(\"\\nFragmentation test:\")\n",
    "for retention in SWEEP_CONFIG['retention_values']:\n",
    "    if retention == 1.0:\n",
    "        frag_net = baseline_network\n",
    "    else:\n",
    "        frag_net = fragment_network(\n",
    "            baseline_network, retention,\n",
    "            method=SWEEP_CONFIG['fragmentation_method'],\n",
    "            seed=42\n",
    "        )\n",
    "    metrics = compute_network_metrics(frag_net)\n",
    "    print(f\"  {retention*100:3.0f}% retention: {metrics['n_edges']:4d} edges, density={metrics['density']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worker-header",
   "metadata": {},
   "source": [
    "## 5. Worker Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worker-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fragmentation_forcing_experiment(args):\n",
    "    \"\"\"\n",
    "    Worker function for a single fragmentation + forcing experiment.\n",
    "    \n",
    "    Parameters are passed as a tuple for compatibility with client.map().\n",
    "    \"\"\"\n",
    "    network_bytes, retention, forcing, config, seed = args\n",
    "    \n",
    "    import sys\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    \n",
    "    if '/opt/research-local/src' not in sys.path:\n",
    "        sys.path.insert(0, '/opt/research-local/src')\n",
    "    \n",
    "    from energy_constrained.solvers import run_two_phase_experiment\n",
    "    from energy_constrained import fragment_network\n",
    "    \n",
    "    # Reconstruct network\n",
    "    baseline_network = pickle.loads(network_bytes)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Apply fragmentation\n",
    "    if retention < 1.0:\n",
    "        network = fragment_network(\n",
    "            baseline_network, retention,\n",
    "            method=config['fragmentation_method'],\n",
    "            seed=seed  # Different fragmentation pattern per run\n",
    "        )\n",
    "    else:\n",
    "        network = baseline_network\n",
    "    \n",
    "    # Run two-phase experiment\n",
    "    result = run_two_phase_experiment(\n",
    "        network=network,\n",
    "        cascade_duration=config['cascade_duration'],\n",
    "        recovery_duration=config['recovery_duration'],\n",
    "        dt=config['dt'],\n",
    "        cascade_sigma=config['cascade_sigma'],\n",
    "        cascade_alpha=config['cascade_alpha'],\n",
    "        recovery_sigma=config['recovery_sigma'],\n",
    "        recovery_alpha=config['recovery_alpha'],\n",
    "        recovery_forcing=forcing,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Compute additional metrics\n",
    "    pct_time_tipped = np.mean(result.x_full > 0) * 100\n",
    "    \n",
    "    n_cells = result.x_full.shape[1]\n",
    "    n_tip_events = 0\n",
    "    n_recover_events = 0\n",
    "    \n",
    "    for j in range(n_cells):\n",
    "        x_traj = result.x_full[:, j]\n",
    "        signs = np.sign(x_traj)\n",
    "        sign_changes = np.diff(signs)\n",
    "        n_tip_events += np.sum(sign_changes > 0)\n",
    "        n_recover_events += np.sum(sign_changes < 0)\n",
    "    \n",
    "    return {\n",
    "        'retention': retention,\n",
    "        'forcing': forcing,\n",
    "        'seed': seed,\n",
    "        'recovery_fraction': result.metrics['recovery_fraction'],\n",
    "        'pct_tipped_cascade': result.metrics['pct_tipped_at_cascade_end'],\n",
    "        'n_permanent_tips': result.metrics['n_permanent_tips'],\n",
    "        'final_pct_tipped': result.metrics['final_pct_tipped'],\n",
    "        'pct_time_tipped': pct_time_tipped,\n",
    "        'n_tip_events': n_tip_events,\n",
    "        'n_recover_events': n_recover_events,\n",
    "        'mean_recovery_time': result.metrics['mean_recovery_time'],\n",
    "    }\n",
    "\n",
    "print(\"Worker function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-header",
   "metadata": {},
   "source": [
    "## 6. Run Two-Dimensional Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize and scatter baseline network\n",
    "network_bytes = pickle.dumps(baseline_network)\n",
    "print(f\"Network serialized: {len(network_bytes) / 1024:.1f} KB\")\n",
    "\n",
    "network_future = client.scatter(network_bytes, broadcast=True)\n",
    "print(\"Network broadcast to all workers\")\n",
    "\n",
    "# Build task arguments\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPERIMENT 11: Starting Fragmentation × Forcing Sweep\")\n",
    "print(\"=\" * 60)\n",
    "start_time = time.time()\n",
    "\n",
    "task_args = []\n",
    "task_idx = 0\n",
    "for r_idx, retention in enumerate(SWEEP_CONFIG['retention_values']):\n",
    "    for f_idx, forcing in enumerate(SWEEP_CONFIG['forcing_values']):\n",
    "        for run_idx in range(SWEEP_CONFIG['n_runs_per_condition']):\n",
    "            seed = SWEEP_CONFIG['base_seed'] + r_idx * 10000 + f_idx * 1000 + run_idx\n",
    "            task_args.append((network_bytes, float(retention), float(forcing), SWEEP_CONFIG, seed))\n",
    "            task_idx += 1\n",
    "\n",
    "print(f\"Generated {len(task_args)} task arguments\")\n",
    "\n",
    "# Submit all tasks at once using client.map()\n",
    "futures = client.map(run_fragmentation_forcing_experiment, task_args)\n",
    "print(f\"Submitted {len(futures)} tasks\")\n",
    "\n",
    "# Collect results with progress tracking\n",
    "all_results = []\n",
    "completed_by_condition = {}\n",
    "\n",
    "print(\"\\nProgress:\")\n",
    "for i, future in enumerate(as_completed(futures)):\n",
    "    result = future.result()\n",
    "    all_results.append(result)\n",
    "    \n",
    "    # Track progress by condition\n",
    "    key = (result['retention'], result['forcing'])\n",
    "    completed_by_condition[key] = completed_by_condition.get(key, 0) + 1\n",
    "    \n",
    "    if (i + 1) % 40 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        rate = (i + 1) / elapsed\n",
    "        remaining = (len(futures) - i - 1) / rate\n",
    "        print(f\"  Completed {i+1}/{len(futures)} ({100*(i+1)/len(futures):.1f}%) \"\n",
    "              f\"- {elapsed:.0f}s elapsed, ~{remaining:.0f}s remaining\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"COMPLETE: {len(all_results)} simulations in {elapsed:.1f}s ({elapsed/60:.1f} min)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## 7. Results Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_results)\n",
    "print(f\"Results shape: {df.shape}\")\n",
    "\n",
    "df = df.sort_values(['retention', 'forcing', 'seed']).reset_index(drop=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by retention and forcing\n",
    "summary = df.groupby(['retention', 'forcing']).agg({\n",
    "    'recovery_fraction': ['mean', 'std', 'min', 'max'],\n",
    "    'n_permanent_tips': ['mean', 'std'],\n",
    "    'pct_tipped_cascade': ['mean', 'std'],\n",
    "    'n_tip_events': ['mean'],\n",
    "    'n_recover_events': ['mean'],\n",
    "}).round(4)\n",
    "\n",
    "summary.columns = ['_'.join(col) for col in summary.columns]\n",
    "summary = summary.reset_index()\n",
    "\n",
    "# Add tip/recovery ratio\n",
    "summary['tip_recovery_ratio'] = summary['n_tip_events_mean'] / summary['n_recover_events_mean']\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"FRAGMENTATION × FORCING SUMMARY\")\n",
    "print(\"=\" * 90)\n",
    "print(summary[['retention', 'forcing', 'recovery_fraction_mean', 'recovery_fraction_std',\n",
    "               'n_permanent_tips_mean', 'tip_recovery_ratio']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## 8. Primary Visualization: Interaction Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Color palette for retention levels\n",
    "colors = plt.cm.viridis(np.linspace(0, 0.8, len(SWEEP_CONFIG['retention_values'])))\n",
    "retention_labels = [f\"{r*100:.0f}%\" for r in SWEEP_CONFIG['retention_values']]\n",
    "\n",
    "# Panel 1: Recovery Fraction vs Forcing (by retention)\n",
    "ax = axes[0, 0]\n",
    "for i, retention in enumerate(SWEEP_CONFIG['retention_values']):\n",
    "    subset = summary[summary['retention'] == retention]\n",
    "    ax.errorbar(\n",
    "        -subset['forcing'].values,\n",
    "        subset['recovery_fraction_mean'].values,\n",
    "        yerr=subset['recovery_fraction_std'].values,\n",
    "        marker='o', capsize=4, linewidth=2, markersize=8,\n",
    "        color=colors[i], label=retention_labels[i]\n",
    "    )\n",
    "ax.axhline(0.5, color='gray', linestyle='--', alpha=0.7, label='50% recovery')\n",
    "ax.set_xlabel('Restoration Forcing Strength (|f|)', fontsize=12)\n",
    "ax.set_ylabel('Recovery Fraction', fontsize=12)\n",
    "ax.set_title('Recovery vs Forcing (by Fragmentation)', fontsize=14)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.legend(title='Edge Retention')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Tip/Recovery Ratio vs Forcing (by retention)\n",
    "ax = axes[0, 1]\n",
    "for i, retention in enumerate(SWEEP_CONFIG['retention_values']):\n",
    "    subset = summary[summary['retention'] == retention]\n",
    "    ax.plot(\n",
    "        -subset['forcing'].values,\n",
    "        subset['tip_recovery_ratio'].values,\n",
    "        marker='s', linewidth=2, markersize=8,\n",
    "        color=colors[i], label=retention_labels[i]\n",
    "    )\n",
    "ax.axhline(1.0, color='gray', linestyle='--', alpha=0.7, label='Symmetric')\n",
    "ax.set_xlabel('Restoration Forcing Strength (|f|)', fontsize=12)\n",
    "ax.set_ylabel('Tip/Recovery Event Ratio', fontsize=12)\n",
    "ax.set_title('Asymmetry vs Forcing (by Fragmentation)', fontsize=14)\n",
    "ax.legend(title='Edge Retention')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 3: Heatmap of Recovery Fraction\n",
    "ax = axes[1, 0]\n",
    "pivot = summary.pivot(index='retention', columns='forcing', values='recovery_fraction_mean')\n",
    "im = ax.imshow(pivot.values, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "ax.set_xticks(range(len(pivot.columns)))\n",
    "ax.set_xticklabels([f\"{-f:.1f}\" for f in pivot.columns])\n",
    "ax.set_yticks(range(len(pivot.index)))\n",
    "ax.set_yticklabels([f\"{r*100:.0f}%\" for r in pivot.index])\n",
    "ax.set_xlabel('Forcing Strength (|f|)', fontsize=12)\n",
    "ax.set_ylabel('Edge Retention', fontsize=12)\n",
    "ax.set_title('Recovery Fraction Heatmap', fontsize=14)\n",
    "\n",
    "# Add values to heatmap\n",
    "for i in range(len(pivot.index)):\n",
    "    for j in range(len(pivot.columns)):\n",
    "        val = pivot.values[i, j]\n",
    "        color = 'white' if val < 0.5 else 'black'\n",
    "        ax.text(j, i, f\"{val:.2f}\", ha='center', va='center', color=color, fontsize=10)\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Recovery Fraction')\n",
    "\n",
    "# Panel 4: Permanent Tips Heatmap\n",
    "ax = axes[1, 1]\n",
    "pivot_tips = summary.pivot(index='retention', columns='forcing', values='n_permanent_tips_mean')\n",
    "im2 = ax.imshow(pivot_tips.values, cmap='Reds', aspect='auto')\n",
    "ax.set_xticks(range(len(pivot_tips.columns)))\n",
    "ax.set_xticklabels([f\"{-f:.1f}\" for f in pivot_tips.columns])\n",
    "ax.set_yticks(range(len(pivot_tips.index)))\n",
    "ax.set_yticklabels([f\"{r*100:.0f}%\" for r in pivot_tips.index])\n",
    "ax.set_xlabel('Forcing Strength (|f|)', fontsize=12)\n",
    "ax.set_ylabel('Edge Retention', fontsize=12)\n",
    "ax.set_title('Permanent Tips Heatmap', fontsize=14)\n",
    "\n",
    "# Add values\n",
    "for i in range(len(pivot_tips.index)):\n",
    "    for j in range(len(pivot_tips.columns)):\n",
    "        val = pivot_tips.values[i, j]\n",
    "        color = 'white' if val > 20 else 'black'\n",
    "        ax.text(j, i, f\"{val:.1f}\", ha='center', va='center', color=color, fontsize=10)\n",
    "\n",
    "plt.colorbar(im2, ax=ax, label='Permanent Tips')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspace/data/exp11_fragmentation_forcing.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nPlot saved to /workspace/data/exp11_fragmentation_forcing.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threshold-header",
   "metadata": {},
   "source": [
    "## 9. Critical Forcing by Retention Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "find-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_critical_forcing(summary_df, retention, recovery_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Find minimum forcing needed to achieve recovery_threshold at given retention.\n",
    "    \"\"\"\n",
    "    subset = summary_df[summary_df['retention'] == retention].sort_values('forcing')\n",
    "    forcings = subset['forcing'].values\n",
    "    recoveries = subset['recovery_fraction_mean'].values\n",
    "    \n",
    "    # Find crossing point\n",
    "    for i in range(len(forcings) - 1):\n",
    "        if recoveries[i] >= recovery_threshold > recoveries[i+1]:\n",
    "            # Linear interpolation\n",
    "            slope = (recoveries[i+1] - recoveries[i]) / (forcings[i+1] - forcings[i])\n",
    "            if slope != 0:\n",
    "                critical = forcings[i] + (recovery_threshold - recoveries[i]) / slope\n",
    "                return critical\n",
    "    \n",
    "    # Check boundary cases\n",
    "    if recoveries.max() < recovery_threshold:\n",
    "        return None  # Never reaches threshold\n",
    "    elif recoveries.min() >= recovery_threshold:\n",
    "        return forcings.max()  # Even weakest forcing is enough\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CRITICAL FORCING BY RETENTION LEVEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "critical_forcings = {}\n",
    "for retention in SWEEP_CONFIG['retention_values']:\n",
    "    critical = find_critical_forcing(summary, retention, 0.50)\n",
    "    critical_forcings[retention] = critical\n",
    "    \n",
    "    if critical is not None:\n",
    "        print(f\"\\n{retention*100:.0f}% retention: |f| = {-critical:.3f} for 50% recovery\")\n",
    "    else:\n",
    "        # Check if always above or below\n",
    "        subset = summary[summary['retention'] == retention]\n",
    "        max_rec = subset['recovery_fraction_mean'].max()\n",
    "        if max_rec >= 0.5:\n",
    "            print(f\"\\n{retention*100:.0f}% retention: 50% recovery achieved even at f=0\")\n",
    "        else:\n",
    "            print(f\"\\n{retention*100:.0f}% retention: 50% recovery NOT achieved (max={max_rec:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forcing-multiplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate forcing multiplier relative to intact network\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FORCING REQUIREMENT MULTIPLIER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "baseline_forcing = critical_forcings.get(1.0)\n",
    "if baseline_forcing is not None:\n",
    "    print(f\"\\nBaseline (100% retention): |f| = {-baseline_forcing:.3f}\")\n",
    "    print(\"\\nMultiplier relative to baseline:\")\n",
    "    for retention in SWEEP_CONFIG['retention_values']:\n",
    "        critical = critical_forcings.get(retention)\n",
    "        if critical is not None and baseline_forcing is not None and baseline_forcing != 0:\n",
    "            multiplier = critical / baseline_forcing\n",
    "            print(f\"  {retention*100:.0f}% retention: {multiplier:.2f}× baseline forcing\")\n",
    "        elif critical is None:\n",
    "            print(f\"  {retention*100:.0f}% retention: N/A (threshold not reached)\")\n",
    "else:\n",
    "    print(\"Baseline (100%) achieves 50% recovery with no forcing\")\n",
    "    for retention in SWEEP_CONFIG['retention_values']:\n",
    "        if retention < 1.0:\n",
    "            critical = critical_forcings.get(retention)\n",
    "            if critical is not None:\n",
    "                print(f\"  {retention*100:.0f}% retention requires |f| = {-critical:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interaction-header",
   "metadata": {},
   "source": [
    "## 10. Interaction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interaction-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit linear models for each retention level\n",
    "print(\"=\" * 70)\n",
    "print(\"LINEAR FORCING-RECOVERY RELATIONSHIP BY RETENTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fits = {}\n",
    "for retention in SWEEP_CONFIG['retention_values']:\n",
    "    subset = summary[summary['retention'] == retention]\n",
    "    forcings = -subset['forcing'].values  # Use |f|\n",
    "    recoveries = subset['recovery_fraction_mean'].values\n",
    "    \n",
    "    # Linear fit\n",
    "    if len(forcings) > 1:\n",
    "        slope, intercept = np.polyfit(forcings, recoveries, 1)\n",
    "        fits[retention] = {'slope': slope, 'intercept': intercept}\n",
    "        print(f\"\\n{retention*100:.0f}% retention:\")\n",
    "        print(f\"  recovery ≈ {slope:.3f} × |f| + {intercept:.3f}\")\n",
    "        print(f\"  Each 0.1 increase in |f| adds {slope*0.1:.1%} recovery\")\n",
    "\n",
    "# Compare slopes\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FORCING EFFECTIVENESS COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "if fits:\n",
    "    baseline_slope = fits.get(1.0, {}).get('slope', 0)\n",
    "    if baseline_slope != 0:\n",
    "        for retention in SWEEP_CONFIG['retention_values']:\n",
    "            if retention in fits:\n",
    "                slope = fits[retention]['slope']\n",
    "                efficiency = slope / baseline_slope * 100\n",
    "                print(f\"  {retention*100:.0f}% retention: {efficiency:.1f}% as effective as intact network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "findings-header",
   "metadata": {},
   "source": [
    "## 11. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT 11: KEY FINDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Best and worst conditions\n",
    "best_idx = summary['recovery_fraction_mean'].idxmax()\n",
    "worst_idx = summary['recovery_fraction_mean'].idxmin()\n",
    "\n",
    "best = summary.loc[best_idx]\n",
    "worst = summary.loc[worst_idx]\n",
    "\n",
    "print(f\"\"\"\n",
    "1. BEST RECOVERY CONDITION:\n",
    "   Retention: {best['retention']*100:.0f}%\n",
    "   Forcing: f = {best['forcing']:.2f}\n",
    "   Recovery: {best['recovery_fraction_mean']:.1%} ± {best['recovery_fraction_std']:.1%}\n",
    "\n",
    "2. WORST RECOVERY CONDITION:\n",
    "   Retention: {worst['retention']*100:.0f}%\n",
    "   Forcing: f = {worst['forcing']:.2f}\n",
    "   Recovery: {worst['recovery_fraction_mean']:.1%} ± {worst['recovery_fraction_std']:.1%}\n",
    "\n",
    "3. FRAGMENTATION-FORCING INTERACTION:\n",
    "\"\"\")\n",
    "\n",
    "# Compare passive recovery across retention levels\n",
    "print(\"   Passive recovery (f=0) by retention:\")\n",
    "for retention in SWEEP_CONFIG['retention_values']:\n",
    "    passive = summary[(summary['retention'] == retention) & (summary['forcing'] == 0)]\n",
    "    if len(passive) > 0:\n",
    "        rec = passive['recovery_fraction_mean'].values[0]\n",
    "        print(f\"     {retention*100:.0f}% retention: {rec:.1%}\")\n",
    "\n",
    "# Compare maximum forcing recovery\n",
    "max_forcing = SWEEP_CONFIG['forcing_values'].min()  # Most negative = strongest\n",
    "print(f\"\\n   Maximum forcing (f={max_forcing}) by retention:\")\n",
    "for retention in SWEEP_CONFIG['retention_values']:\n",
    "    max_f = summary[(summary['retention'] == retention) & (summary['forcing'] == max_forcing)]\n",
    "    if len(max_f) > 0:\n",
    "        rec = max_f['recovery_fraction_mean'].values[0]\n",
    "        print(f\"     {retention*100:.0f}% retention: {rec:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/workspace/data/experiment11_fragmentation_forcing.csv', index=False)\n",
    "print(f\"Full results saved to /workspace/data/experiment11_fragmentation_forcing.csv\")\n",
    "\n",
    "summary.to_csv('/workspace/data/experiment11_summary.csv', index=False)\n",
    "print(f\"Summary saved to /workspace/data/experiment11_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT 11 COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "CONFIGURATION:\n",
    "- Retention levels: {SWEEP_CONFIG['retention_values']}\n",
    "- Forcing values: {list(SWEEP_CONFIG['forcing_values'])}\n",
    "- Runs per condition: {SWEEP_CONFIG['n_runs_per_condition']}\n",
    "- Total simulations: {len(df)}\n",
    "- Runtime: {elapsed:.1f}s ({elapsed/60:.1f} min)\n",
    "\n",
    "FILES GENERATED:\n",
    "- /workspace/data/experiment11_fragmentation_forcing.csv\n",
    "- /workspace/data/experiment11_summary.csv\n",
    "- /workspace/data/exp11_fragmentation_forcing.png\n",
    "\n",
    "KEY CONCLUSIONS:\n",
    "- [Fill after running experiment]\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "# Experiment 12b: Keystone Edge Protection\n\n**Phase 4 - Testing Targeted Conservation Strategies**\n\n## Background\n\nExperiment 12 revealed that **82% of edge removals IMPROVE recovery** - a counterintuitive finding suggesting most network connections actually hinder recovery. Only **6% of edges are true \"keystones\"** whose removal significantly harms recovery capacity.\n\n## Key Question\n\n**Does protecting ONLY the keystone edges (6%) provide equivalent recovery benefits to protecting the entire network?**\n\nIf so, this has major conservation implications: resources can be concentrated on a small subset of critical connections.\n\n## Experimental Design\n\n| Condition | Description | N Edges |\n|-----------|-------------|--------|\n| `full_network` | All edges intact (baseline) | ~100 |\n| `keystone_only` | Only keystone edges (top 6 harmful) | 6 |\n| `random_6` | Random 6 edges preserved | 6 |\n| `random_10pct` | Random 10% of edges preserved | ~10 |\n| `top_flow_6` | Top 6 edges by flow preserved | 6 |\n| `no_edges` | All edges removed (isolated cells) | 0 |\n\n## Protocol\n\n**NO FORCING** - matches Experiment 12 conditions exactly.\n- Cascade phase: Lévy noise (α=1.5, σ=0.06) triggers tipping\n- Recovery phase: Gaussian noise (α=2.0, σ=0.04) allows potential recovery\n\n## Expected Outcomes\n\n- `keystone_only` should have recovery close to `full_network`\n- `random_6` and `top_flow_6` should have much lower recovery\n- This would validate the keystone identification from Experiment 12"
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/opt/research-local/src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "from netCDF4 import Dataset\n",
    "from dask.distributed import as_completed\n",
    "\n",
    "from energy_constrained import get_dask_client\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dask-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Dask cluster\n",
    "client = get_dask_client()\n",
    "print(f\"Connected to: {client.scheduler_info()['address']}\")\n",
    "print(f\"Workers: {len(client.scheduler_info()['workers'])}\")\n",
    "print(f\"Total threads: {sum(w['nthreads'] for w in client.scheduler_info()['workers'].values())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## 2. Load Amazon Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('/opt/research-local/data/amazon/amazon_adaptation_model/average_network/era5_new_network_data')\n",
    "\n",
    "def load_amazon_data(year=2003, months=[7, 8, 9]):\n",
    "    \"\"\"Load and average Amazon moisture recycling data.\"\"\"\n",
    "    all_rain = []\n",
    "    all_evap = []\n",
    "    all_network = []\n",
    "    \n",
    "    for month in months:\n",
    "        file_path = DATA_PATH / f'1deg_{year}_{month:02d}.nc'\n",
    "        if file_path.exists():\n",
    "            with Dataset(file_path, 'r') as ds:\n",
    "                all_rain.append(ds.variables['rain'][:])\n",
    "                all_evap.append(ds.variables['evap'][:])\n",
    "                all_network.append(ds.variables['network'][:])\n",
    "    \n",
    "    return {\n",
    "        'rain': np.mean(all_rain, axis=0),\n",
    "        'evap': np.mean(all_evap, axis=0),\n",
    "        'network': np.mean(all_network, axis=0),\n",
    "        'n_cells': len(all_rain[0])\n",
    "    }\n",
    "\n",
    "amazon_data = load_amazon_data(year=2003)\n",
    "print(f\"Loaded Amazon data: {amazon_data['n_cells']} cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 3. Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": "CONFIG = {\n    'n_cells': 50,\n    'min_flow': 1.0,\n    'barrier_height': 0.2,\n    'cascade_duration': 200,\n    'recovery_duration': 800,\n    'dt': 0.5,\n    'cascade_sigma': 0.06,\n    'cascade_alpha': 1.5,\n    'recovery_sigma': 0.04,\n    'recovery_alpha': 2.0,\n    'forcing': 0.0,  # NO FORCING - matches Experiment 12 conditions\n    'n_runs': 20,\n    'base_seed': 42,\n}\n\n# Keystone edges from Experiment 12 (most critical for recovery)\n# These are the edges whose removal HURTS recovery the most\nKEYSTONE_EDGES = [\n    ('cell_28', 'cell_14'),  # Most critical: -7.9% recovery impact\n    ('cell_27', 'cell_12'),  # Second: from Exp 12 top 20 list\n    ('cell_32', 'cell_40'),\n    ('cell_31', 'cell_4'),\n    ('cell_17', 'cell_32'),\n    ('cell_31', 'cell_24'),\n]\n\nKEYSTONE_NODES = ['cell_14', 'cell_31', 'cell_27', 'cell_32', 'cell_28', 'cell_40', 'cell_12']\n\nprint(\"=\" * 60)\nprint(\"EXPERIMENT 12b: KEYSTONE PROTECTION TEST\")\nprint(\"=\" * 60)\nprint(f\"Runs per condition: {CONFIG['n_runs']}\")\nprint(f\"Forcing: {CONFIG['forcing']} (NO FORCING - matches Exp 12)\")\nprint(f\"Keystone edges: {len(KEYSTONE_EDGES)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "network-header",
   "metadata": {},
   "source": [
    "## 4. Build Edge List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "network-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_edge_list(data, config, seed=42):\n",
    "    \"\"\"Get full list of edges from Amazon network.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    network_matrix = data['network']\n",
    "    n_cells = config['n_cells']\n",
    "    min_flow = config['min_flow']\n",
    "    \n",
    "    total_flow = network_matrix.sum(axis=0) + network_matrix.sum(axis=1)\n",
    "    top_indices = np.argsort(total_flow)[-n_cells:]\n",
    "    \n",
    "    edges = []\n",
    "    for i, idx_i in enumerate(top_indices):\n",
    "        for j, idx_j in enumerate(top_indices):\n",
    "            if i != j:\n",
    "                flow = network_matrix[idx_i, idx_j]\n",
    "                if flow > min_flow:\n",
    "                    edges.append((f'cell_{i}', f'cell_{j}', flow))\n",
    "    \n",
    "    return edges, top_indices\n",
    "\n",
    "full_edges, top_indices = get_full_edge_list(amazon_data, CONFIG)\n",
    "print(f\"Total edges in network: {len(full_edges)}\")\n",
    "print(f\"Keystone edges to protect: {len(KEYSTONE_EDGES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditions-header",
   "metadata": {},
   "source": [
    "## 5. Define Experimental Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_edge_keys = [(e[0], e[1]) for e in full_edges]\n",
    "edges_by_flow = sorted(full_edges, key=lambda x: x[2], reverse=True)\n",
    "top_flow_edges = [(e[0], e[1]) for e in edges_by_flow[:6]]\n",
    "\n",
    "np.random.seed(CONFIG['base_seed'])\n",
    "random_6_edges = [all_edge_keys[i] for i in np.random.choice(len(all_edge_keys), 6, replace=False)]\n",
    "random_10pct_edges = [all_edge_keys[i] for i in np.random.choice(len(all_edge_keys), \n",
    "                                                                   max(1, len(all_edge_keys)//10), \n",
    "                                                                   replace=False)]\n",
    "\n",
    "CONDITIONS = {\n",
    "    'full_network': {'edges': 'all', 'description': 'All edges intact (baseline)'},\n",
    "    'keystone_only': {'edges': KEYSTONE_EDGES, 'description': f'Only {len(KEYSTONE_EDGES)} keystone edges'},\n",
    "    'random_6': {'edges': random_6_edges, 'description': 'Random 6 edges'},\n",
    "    'random_10pct': {'edges': random_10pct_edges, 'description': f'Random {len(random_10pct_edges)} edges (10%)'},\n",
    "    'top_flow_6': {'edges': top_flow_edges, 'description': 'Top 6 edges by flow'},\n",
    "    'no_edges': {'edges': 'none', 'description': 'No edges (isolated cells)'},\n",
    "}\n",
    "\n",
    "print(\"EXPERIMENTAL CONDITIONS:\")\n",
    "print(\"=\" * 60)\n",
    "for name, cond in CONDITIONS.items():\n",
    "    n_edges = len(full_edges) if cond['edges'] == 'all' else (0 if cond['edges'] == 'none' else len(cond['edges']))\n",
    "    print(f\"  {name:20} : {n_edges:3} edges - {cond['description']}\")\n",
    "\n",
    "print(f\"\\nTotal simulations: {len(CONDITIONS) * CONFIG['n_runs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worker-header",
   "metadata": {},
   "source": [
    "## 6. Worker Function (Standalone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worker-function",
   "metadata": {},
   "outputs": [],
   "source": "def run_protection_experiment(data_bytes, condition_name, edges_to_keep, config, seed):\n    \"\"\"\n    Worker function using the actual run_two_phase_experiment from solvers.\n    This ensures we match Experiment 12's methodology exactly.\n    \"\"\"\n    import numpy as np\n    import pickle\n    import sys\n    \n    if '/opt/research-local/src' not in sys.path:\n        sys.path.insert(0, '/opt/research-local/src')\n    \n    from energy_constrained import (\n        EnergyConstrainedNetwork,\n        EnergyConstrainedCusp,\n        GradientDrivenCoupling,\n    )\n    from energy_constrained.solvers import run_two_phase_experiment\n    \n    # Deserialize data\n    data = pickle.loads(data_bytes)\n    np.random.seed(seed)\n    \n    # Extract parameters\n    network_matrix = data['network']\n    n_cells = config['n_cells']\n    min_flow = config['min_flow']\n    barrier_height = config['barrier_height']\n    \n    # Select top cells by total flow\n    total_flow = network_matrix.sum(axis=0) + network_matrix.sum(axis=1)\n    top_indices = np.argsort(total_flow)[-n_cells:]\n    \n    # Build network using EnergyConstrainedNetwork\n    net = EnergyConstrainedNetwork()\n    \n    # Add elements\n    for i in range(n_cells):\n        element = EnergyConstrainedCusp(\n            a=-1.0, b=1.0, c=0.0, x_0=0.0,\n            barrier_height=barrier_height,\n            dissipation_rate=0.1\n        )\n        net.add_element(f'cell_{i}', element)\n    \n    # Build all possible edges\n    all_edges = {}\n    for i, idx_i in enumerate(top_indices):\n        for j, idx_j in enumerate(top_indices):\n            if i != j:\n                flow = network_matrix[idx_i, idx_j]\n                if flow > min_flow:\n                    all_edges[(f'cell_{i}', f'cell_{j}')] = flow\n    \n    # Determine which edges to keep\n    if edges_to_keep == 'all':\n        edges_set = set(all_edges.keys())\n    elif edges_to_keep == 'none':\n        edges_set = set()\n    else:\n        edges_set = set(tuple(e) for e in edges_to_keep)\n    \n    # Add couplings for kept edges\n    n_edges_added = 0\n    for (src, tgt), flow in all_edges.items():\n        if (src, tgt) in edges_set:\n            coupling = GradientDrivenCoupling(\n                conductivity=flow / 100.0,\n                state_coupling=0.1\n            )\n            net.add_coupling(src, tgt, coupling)\n            n_edges_added += 1\n    \n    # Run two-phase experiment using the SAME function as Experiment 12\n    result = run_two_phase_experiment(\n        network=net,\n        cascade_duration=config['cascade_duration'],\n        recovery_duration=config['recovery_duration'],\n        dt=config['dt'],\n        cascade_sigma=config['cascade_sigma'],\n        cascade_alpha=config['cascade_alpha'],\n        recovery_sigma=config['recovery_sigma'],\n        recovery_alpha=config['recovery_alpha'],\n        seed=seed\n    )\n    \n    # Extract metrics (same as Exp 12)\n    n_cells_actual = result.x_full.shape[1]\n    n_tip_events = 0\n    n_recover_events = 0\n    \n    for j in range(n_cells_actual):\n        x_traj = result.x_full[:, j]\n        signs = np.sign(x_traj)\n        sign_changes = np.diff(signs)\n        n_tip_events += np.sum(sign_changes > 0)\n        n_recover_events += np.sum(sign_changes < 0)\n    \n    tip_recovery_ratio = n_tip_events / n_recover_events if n_recover_events > 0 else np.nan\n    \n    return {\n        'condition': condition_name,\n        'n_edges': n_edges_added,\n        'seed': seed,\n        'pct_tipped_cascade': result.metrics['pct_tipped_at_cascade_end'],\n        'final_pct_tipped': result.metrics['final_pct_tipped'],\n        'recovery_fraction': result.metrics['recovery_fraction'],\n        'n_tip_events': n_tip_events,\n        'n_recover_events': n_recover_events,\n        'tip_recovery_ratio': tip_recovery_ratio,\n        'n_permanent_tips': result.metrics['n_permanent_tips'],\n    }\n\nprint(\"Worker function defined (using run_two_phase_experiment).\")"
  },
  {
   "cell_type": "markdown",
   "id": "run-header",
   "metadata": {},
   "source": [
    "## 7. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bytes = pickle.dumps(amazon_data)\n",
    "print(f\"Data serialized: {len(data_bytes) / 1024:.1f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPERIMENT 12b: Starting Keystone Protection Test\")\n",
    "print(\"=\" * 60)\n",
    "start_time = time.time()\n",
    "\n",
    "futures = []\n",
    "for condition_name, condition in CONDITIONS.items():\n",
    "    edges = condition['edges']\n",
    "    for run_idx in range(CONFIG['n_runs']):\n",
    "        seed = CONFIG['base_seed'] + hash(condition_name) % 10000 + run_idx\n",
    "        future = client.submit(\n",
    "            run_protection_experiment,\n",
    "            data_bytes, condition_name, edges, CONFIG, seed\n",
    "        )\n",
    "        futures.append(future)\n",
    "\n",
    "print(f\"Submitted {len(futures)} tasks\")\n",
    "\n",
    "all_results = []\n",
    "print(\"\\nProgress:\")\n",
    "for i, future in enumerate(as_completed(futures)):\n",
    "    result = future.result()\n",
    "    all_results.append(result)\n",
    "    \n",
    "    if (i + 1) % 20 == 0 or (i + 1) == len(futures):\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  Completed {i+1}/{len(futures)} ({100*(i+1)/len(futures):.1f}%) - {elapsed:.0f}s elapsed\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"COMPLETE: {len(all_results)} simulations in {elapsed:.1f}s ({elapsed/60:.1f} min)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-header",
   "metadata": {},
   "source": [
    "## 8. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_results)\n",
    "print(f\"Results shape: {df.shape}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df.groupby('condition').agg({\n",
    "    'n_edges': 'first',\n",
    "    'recovery_fraction': ['mean', 'std'],\n",
    "    'pct_tipped_cascade': ['mean', 'std'],\n",
    "    'final_pct_tipped': ['mean', 'std'],\n",
    "    'tip_recovery_ratio': ['mean', 'std'],\n",
    "}).round(4)\n",
    "\n",
    "summary.columns = ['_'.join(col) if col[1] else col[0] for col in summary.columns]\n",
    "summary = summary.reset_index()\n",
    "\n",
    "condition_order = ['full_network', 'keystone_only', 'random_6', 'random_10pct', 'top_flow_6', 'no_edges']\n",
    "summary['order'] = summary['condition'].map({c: i for i, c in enumerate(condition_order)})\n",
    "summary = summary.sort_values('order').drop('order', axis=1)\n",
    "\n",
    "print(\"SUMMARY BY CONDITION:\")\n",
    "print(\"=\" * 80)\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## 9. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "colors = {\n",
    "    'full_network': 'green',\n",
    "    'keystone_only': 'blue',\n",
    "    'random_6': 'orange',\n",
    "    'random_10pct': 'purple',\n",
    "    'top_flow_6': 'red',\n",
    "    'no_edges': 'gray',\n",
    "}\n",
    "\n",
    "# Panel 1: Recovery Fraction\n",
    "ax = axes[0, 0]\n",
    "x_pos = range(len(condition_order))\n",
    "means = [summary[summary['condition'] == c]['recovery_fraction_mean'].values[0] for c in condition_order]\n",
    "stds = [summary[summary['condition'] == c]['recovery_fraction_std'].values[0] for c in condition_order]\n",
    "bar_colors = [colors[c] for c in condition_order]\n",
    "\n",
    "ax.bar(x_pos, means, yerr=stds, capsize=5, color=bar_colors, edgecolor='black', alpha=0.8)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([c.replace('_', '\\n') for c in condition_order], fontsize=10)\n",
    "ax.set_ylabel('Recovery Fraction', fontsize=12)\n",
    "ax.set_title('Recovery by Protection Strategy', fontsize=14)\n",
    "ax.axhline(means[0], color='green', linestyle='--', alpha=0.5, label='Full network baseline')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.legend()\n",
    "\n",
    "# Panel 2: Box plot\n",
    "ax = axes[0, 1]\n",
    "box_data = [df[df['condition'] == c]['recovery_fraction'].values for c in condition_order]\n",
    "bp = ax.boxplot(box_data, labels=[c.replace('_', '\\n') for c in condition_order], patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], bar_colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax.set_ylabel('Recovery Fraction', fontsize=12)\n",
    "ax.set_title('Recovery Distribution by Strategy', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Panel 3: Cascade vs Final\n",
    "ax = axes[1, 0]\n",
    "cascade_means = [summary[summary['condition'] == c]['pct_tipped_cascade_mean'].values[0] for c in condition_order]\n",
    "final_means = [summary[summary['condition'] == c]['final_pct_tipped_mean'].values[0] for c in condition_order]\n",
    "\n",
    "x = np.arange(len(condition_order))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, cascade_means, width, label='End of Cascade', color='tomato', edgecolor='black')\n",
    "ax.bar(x + width/2, final_means, width, label='Final', color='steelblue', edgecolor='black')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([c.replace('_', '\\n') for c in condition_order], fontsize=10)\n",
    "ax.set_ylabel('% Cells Tipped', fontsize=12)\n",
    "ax.set_title('Cascade Impact and Final State', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Panel 4: Tip/Recovery Ratio\n",
    "ax = axes[1, 1]\n",
    "ratio_means = [summary[summary['condition'] == c]['tip_recovery_ratio_mean'].values[0] for c in condition_order]\n",
    "ratio_stds = [summary[summary['condition'] == c]['tip_recovery_ratio_std'].values[0] for c in condition_order]\n",
    "\n",
    "ax.bar(x_pos, ratio_means, yerr=ratio_stds, capsize=5, color=bar_colors, edgecolor='black', alpha=0.8)\n",
    "ax.axhline(1.0, color='red', linestyle='--', label='Symmetric (ratio=1)')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([c.replace('_', '\\n') for c in condition_order], fontsize=10)\n",
    "ax.set_ylabel('Tip/Recovery Event Ratio', fontsize=12)\n",
    "ax.set_title('Tipping Asymmetry by Strategy', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/workspace/data/exp12b_keystone_protection.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nPlot saved to /workspace/data/exp12b_keystone_protection.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "findings-header",
   "metadata": {},
   "source": [
    "## 10. Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT 12b: KEY FINDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "full_recovery = summary[summary['condition'] == 'full_network']['recovery_fraction_mean'].values[0]\n",
    "keystone_recovery = summary[summary['condition'] == 'keystone_only']['recovery_fraction_mean'].values[0]\n",
    "random_recovery = summary[summary['condition'] == 'random_6']['recovery_fraction_mean'].values[0]\n",
    "topflow_recovery = summary[summary['condition'] == 'top_flow_6']['recovery_fraction_mean'].values[0]\n",
    "no_edge_recovery = summary[summary['condition'] == 'no_edges']['recovery_fraction_mean'].values[0]\n",
    "\n",
    "keystone_efficiency = keystone_recovery / full_recovery * 100 if full_recovery > 0 else 0\n",
    "random_efficiency = random_recovery / full_recovery * 100 if full_recovery > 0 else 0\n",
    "topflow_efficiency = topflow_recovery / full_recovery * 100 if full_recovery > 0 else 0\n",
    "\n",
    "print(f\"\"\"\n",
    "1. RECOVERY COMPARISON:\n",
    "   Full network (baseline):  {full_recovery:.1%}\n",
    "   Keystone only (6 edges):  {keystone_recovery:.1%}  ({keystone_efficiency:.0f}% of full)\n",
    "   Random 6 edges:           {random_recovery:.1%}  ({random_efficiency:.0f}% of full)\n",
    "   Top flow 6 edges:         {topflow_recovery:.1%}  ({topflow_efficiency:.0f}% of full)\n",
    "   No edges:                 {no_edge_recovery:.1%}\n",
    "\n",
    "2. KEYSTONE EFFECTIVENESS:\n",
    "   Keystone edges achieve {keystone_efficiency:.0f}% of full network recovery\n",
    "   with only {len(KEYSTONE_EDGES)} edges ({100*len(KEYSTONE_EDGES)/len(full_edges):.1f}% of total)\n",
    "   \n",
    "   Efficiency gain: {keystone_efficiency / (100*len(KEYSTONE_EDGES)/len(full_edges)):.1f}x better than random\n",
    "\n",
    "3. CONSERVATION IMPLICATION:\n",
    "   {'Keystone protection is HIGHLY EFFICIENT!' if keystone_efficiency > 50 else 'Keystone protection shows limited benefit.'}\n",
    "   Focus protection on: {', '.join(KEYSTONE_NODES[:5])}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/workspace/data/experiment12b_keystone_protection_full.csv', index=False)\n",
    "print(f\"Full results saved to /workspace/data/experiment12b_keystone_protection_full.csv\")\n",
    "\n",
    "summary.to_csv('/workspace/data/experiment12b_keystone_protection_summary.csv', index=False)\n",
    "print(f\"Summary saved to /workspace/data/experiment12b_keystone_protection_summary.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPERIMENT 12b COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}